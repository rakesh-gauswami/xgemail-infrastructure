#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4
#
# This script provides a communication between Postfix and S3. It works as a local
# delivery agent to submit server which accepts an email from postfix and upload it
# to a designated S3 bucket. In case of a failure email stays in postfix deferred
# queue and postfix retries later.
#
# Copyright: Copyright (c) 1997-2018. All rights reserved.
# Company: Sophos Limited or one of its affiliates.

import sys
sys.path.append("<%= @xgemail_utils_path %>")

import copy
import formatterutils
import gzip
import gziputils
import io
import json
import logging
import messageformatter
import messagehistoryformatter
import metadataformatter
import multipolicyreaderutils
import os
import re
import signal
import email
import uuidutils
import rfxrecoveryutils
from email.parser import HeaderParser
from awshandler import AwsHandler
from common.metadata import Metadata
from common.sqsmessage import SqsMessage
from common.messagehistoryevent import MessageHistoryEvent
from datetime import datetime
from datetime import timedelta
from email.parser import Parser
from email.utils import parseaddr
from logging.handlers import SysLogHandler

# Constants
AWS_REGION = "<%= @sqs_msg_producer_aws_region %>"
MSG_HISTORY_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_s3_bucket_name %>"
MSG_HISTORY_MS_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_ms_s3_bucket_name %>"
MSG_HISTORY_EVENTS_TOPIC_ARN = "<%= @sns_msg_history_events_sns_topic_arn %>"
PROCESS_TIMEOUT_SECONDS = <%= @sqs_msg_producer_process_timeout_seconds %>

SUBMIT_TYPE = "<%= @xgemail_submit_type %>"

S3_ENCRYPTION_ALGORITHM = "<%= @s3_encryption_algorithm %>"
SUBMIT_BUCKET_NAME = "<%= @sqs_msg_producer_s3_bucket_name %>"
CUSTOMER_SUBMIT_BUCKET_NAME = "<%= @sqs_msg_producer_s3_customer_submit_bucket_name %>"
POLICY_S3_BUCKET_NAME = "<%= @sqs_msg_producer_policy_s3_bucket_name %>"

SUBMIT_HOST_IP = "<%= @sqs_msg_producer_submit_ip %>"
SUBMIT_SQS_URL = "<%= @sqs_msg_producer_sqs_url %>"
CUSTOMER_SUBMIT_SQS_URL = "<%= @sqs_msg_producer_customer_submit_sqs_url %>"

TTL_IN_DAYS = <%= @sqs_msg_producer_ttl_in_days %>

# Exit codes from sysexits
EX_TEMPFAIL = <%= @sqs_msg_producer_ex_temp_failure_code %>

#Message and metadata file details
BUFFER_SIZE = <%= @sqs_msg_producer_buffer_size %>
ROOT_DIR = "<%= @sqs_msg_producer_email_root_dir %>"
TIMESTAMP_FORMAT = "%Y/%m/%d/%H/%M"

# logging to syslog setup
logging.getLogger("botocore").setLevel(logging.WARNING)
logger = logging.getLogger('sqsmsgproducer')
logger.setLevel(logging.INFO)
handler = logging.handlers.SysLogHandler('/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(levelname)s %(message)s'
)
handler.formatter = formatter
logger.addHandler(handler)

# Encryption constants
# TODO: Need to be changed when Encryption is in place
AKM_KEY = "akm_key"
MESSAGE_KEY = "message_key"
NONCE = "nonce"
DOMAIN_NAME_REGEX_PATTERN = "[a-zA-Z0-9][-a-zA-Z0-9]*(\\.[-a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}"
EMAIL_ADDRESS_REGEX_PATTERN = "(([^<>()\[\]\\.,;:\s@\"]+(\.[^<>()\[\]\\.,;:\s@\"]+)*)|(\".+\"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))";

# Other constants
DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
MESSAGE_HISTORY_UNKNOWN_DESIGNATION = "UNKNOWN"
MESSAGE_HISTORY_ACCEPTED_EVENT = "ACCEPTED"
INBOUND_MESSAGE_DIRECTION = "INBOUND"
OUTBOUND_MESSAGE_DIRECTION = "OUTBOUND"

# Submit message types to be transmitted to the appropriate PIC for encrypted messages
SQS_MESSAGE_SUBMIT_TYPE_INBOUND = "INTERNET"
SQS_MESSAGE_SUBMIT_TYPE_OUTBOUND = "CUSTOMER"

# Definition of the header "X-Sophos-Deliver-Inbound: true" for the email package
DELIVER_INBOUND_HEADER = "X-Sophos-Deliver-Inbound"
DELIVER_INBOUND_VALUE = "true"
SOPHOS_PARENT_ID_HEADER = "X-Sophos-MH-Parent-Id"
ECHOWORX_REPLY_HEADER = "x-echoworx-portal"

awshandler = AwsHandler(AWS_REGION)

#Strip the dot at the end of sender and/or recipients addresses as rfc:1034 mentions to consider ending dot optional
def strip_dot_at_end(email_address):
  if email_address.endswith("."):
    return email_address[:-1]
  else:
    return email_address

#postfix pipe sends metadata as sysargs.
def get_metadata(message_headers):
    try:
        metadata_length = len(sys.argv)
        if (metadata_length < 7):
            logger.info(" Usage: xgemail_sqs_message_producer.py <null_sender> "+
                         "<sender> <client_address> <queue_id> <domain> <original_recipient> ")
            exit(EX_TEMPFAIL)

        null_sender = sys.argv[1]
        sender_address = strip_dot_at_end(sys.argv[2])
        sender_ip = sys.argv[3]
        queue_id = sys.argv[4]
        recipient_domain = sys.argv[5]
        # TODO: see if we can pipe arrival date from postfix
        date_recorded = datetime.utcnow().strftime(DATETIME_FORMAT)
        recipient_list = [ ]
        for i in range (6,len(sys.argv)):
            recipient_list.append(strip_dot_at_end(sys.argv[i]))

        if (sender_address == null_sender):
            sender_address = None

        x_sophos_header = uuidutils.get_x_sophos_email_id(message_headers['X-Sophos-Email-ID'], queue_id)

        metadata = Metadata(metadataformatter.SCHEMA_VERSION,
                            sender_ip,
                            sender_address,
                            SUBMIT_HOST_IP,
                            queue_id,
                            date_recorded,
                            recipient_domain,
                            recipient_list,
                            x_sophos_header)

        # required logging for EncryptionServiceTest automation test
        logger.info("Input email metadata info [{0}]".format(metadata))
        return metadata
    except Exception as e:
        logger.exception("Failed in preparing metadata [{0}]".format(e))
        exit(EX_TEMPFAIL)

def add_to_sqs(sqs_url, sqs_json):
    return awshandler.add_to_sqs(
        sqs_url,
        json.dumps(sqs_json)
    )

def upload_to_s3(bucket_name, file_path, formatted_data, expires):
    return awshandler.upload_data_in_s3(
        bucket_name,
        file_path,
        formatted_data,
        expires,
        S3_ENCRYPTION_ALGORITHM
    )

# uploads message file to S3
def upload_message_to_s3(s3_file_path, expires, compressed_message, direction):
    try:
        message_file_path = messageformatter.get_s3_message_path(
            s3_file_path
        )

        # required logging for XgemailTestBase automation test
        logger.info("Processing message [{0}]".format(message_file_path))

        #TODO: Encrypt mime message bytes after V1

        formatted_email_data = messageformatter.get_formatted_email_data(
            compressed_message
        )

        # upload message to the submit bucket
        upload_to_s3(
            get_submit_bucket_name(direction),
            message_file_path,
            formatted_email_data,
            expires
        )

        logger.debug("Uploaded message to S3 [{0}]".format(message_file_path))

    except Exception as e:
        logger.exception("Failed in uploading message to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


def upload_metadata_to_s3(s3_file_path, expires, metadata, direction):
    try:
        metadata_file_path = metadataformatter.get_s3_metadata_path(
            s3_file_path
        )
        logger.debug("Processing metadata [{0}]".format(metadata_file_path))

        #TODO: Encrypt metadata object after V1

        # metadata_magic_bytes, schema_version, nonce_length, gzip_metadata_json):
        formatted_metadata = metadataformatter.get_formatted_metadata(
            gziputils.gzip_data(
                json.dumps(metadata.get_metadata_json())
            )
        )

        # upload metadata to submit bucket
        upload_to_s3(
            get_submit_bucket_name(direction),
            metadata_file_path,
            formatted_metadata,
            expires
        )

        logger.debug("Uploaded metadata to S3 [{0}]".format(metadata_file_path))

    except Exception as e:
        logger.exception("Failed in uploading metadata to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)

# Retrieves the S3 bucket depending on the direction (inbound or outbound).
# This logic is used on the encryption-submit instance only and there are two
# S3 buckets defined on this instance. For the rest instances there is no direction
# switching, so the SUBMIT_BUCKET_NAME is used only, which has the specific value
# for the particular instance.
def get_submit_bucket_name(direction):
    if direction != INBOUND_MESSAGE_DIRECTION:
      return CUSTOMER_SUBMIT_BUCKET_NAME
    else:
      return SUBMIT_BUCKET_NAME

# uploads message history parent file to message history bucket
def upload_msg_history_to_s3(s3_file_path, expires, metadata):
    try:
        msg_history_file_path = messagehistoryformatter.get_s3_msg_history_path(
            s3_file_path
        )
        logger.debug("Processing message history [{0}]".format(msg_history_file_path))

        #TODO: Encrypt msg history object after V1

        # metadata_magic_bytes, schema_version, nonce_length, gzip_metadata_json):
        formatted_msg_history = messagehistoryformatter.get_formatted_msg_history(
            gziputils.gzip_data(
                json.dumps(metadata.get_metadata_json())
            )
        )

        # upload msg history to message history bucket
        upload_to_s3(
            MSG_HISTORY_BUCKET_NAME,
            msg_history_file_path,
            formatted_msg_history,
            expires
        )

        # upload msg history to message history ms bucket
        upload_to_s3(
            MSG_HISTORY_MS_BUCKET_NAME,
            msg_history_file_path,
            formatted_msg_history,
            expires
        )

        logger.debug("Uploaded message history to S3 [{0}]".format(msg_history_file_path))

    except Exception as e:
        logger.exception("Failed in uploading message history to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


# Send an accepted message history event to msg history SQS
def prepare_msg_history_event(s3_file_path, metadata, sqs_message, direction, sender):
    try:
        # create a message history event object
        msg_history_event = MessageHistoryEvent(
            messagehistoryformatter.SCHEMA_VERSION,
            s3_file_path,
            metadata.accepting_server_ip,
            metadata.queue_id,
            sqs_message.akm_key,
            sqs_message.nonce,
            sqs_message.message_key,
            datetime.utcnow().strftime(DATETIME_FORMAT),
            MESSAGE_HISTORY_ACCEPTED_EVENT,
            MESSAGE_HISTORY_UNKNOWN_DESIGNATION,
            None,
            metadata.recipients,
            False,
            direction,
            sender
        )

        return msg_history_event

    except Exception as e:
        logger.exception("Failed in uploading message history event to SQS [{0}]".format(e))
        exit(EX_TEMPFAIL)

def publish_msg_history_event(history_json):
    try:
        logger.debug("Processing message history event SNS job [{0}]".format(history_json))
        awshandler.publish_to_sns_topic(
            MSG_HISTORY_EVENTS_TOPIC_ARN,
            json.dumps(history_json)
        )
    except Exception as e:
        logger.exception("Failed in submit message history event to SNS [{0}]".format(e))
        exit(EX_TEMPFAIL)

# sends msg processing sqs messages to sqs
def send_msg_processing_sqs_message(sqs_message_json, direction, customer_id = None):

    receiving_queue = get_sqs_url(direction)

    try:
        logger.debug("Processing SQS job [{0}] in queue [{1}]".format(sqs_message_json, receiving_queue))
        add_to_sqs(receiving_queue, sqs_message_json)
    except Exception as e:
        logger.exception("Failed in uploading message processing SQS job [{0}]".format(e))
        exit(EX_TEMPFAIL)

# Retrieves the SQS URL depending on the direction (inbound or outbound).
# This logic is used on the encryption-submit instance only and there are two
# SQS URLs defined on this instance. For the rest instances there is no direction
# switching, so the SUBMIT_SQS_URL is used only, which has the specific value
# for the particular instance.
def get_sqs_url(direction):
    if direction != INBOUND_MESSAGE_DIRECTION:
      return CUSTOMER_SUBMIT_SQS_URL
    else:
      return SUBMIT_SQS_URL

def get_plaintext_message():
    stdin_no = sys.stdin.fileno()
    plaintext_email_bytes = io.BytesIO()

    try:
        while True:
            try:
                new_bytes = os.read(stdin_no, BUFFER_SIZE)
                if not new_bytes:
                    break
                plaintext_email_bytes.write(new_bytes)
            except EOFError:
                break
            except Exception as e:
                logger.exception("Failed in parsing input email [{0}]".format(e))
                exit(EX_TEMPFAIL)
            except BaseException as e:
                logger.exception("Unexpected error in parsing input email: [{0}]".format(e))
                exit(EX_TEMPFAIL)
        return plaintext_email_bytes.getvalue()
    finally:
        plaintext_email_bytes.close()

def get_metadata_for_outbound(metadata, message_headers):
    effective_sender_value = message_headers['x-sophos-effective-sender']
    effective_sender_header = 'x-sophos-effective-sender'

    if effective_sender_value is not None:
        return get_metadata_for_outbound_effective_sender(metadata, effective_sender_header, effective_sender_value)

    from_header = message_headers['from']
    if from_header is None:
        raise Exception("Invalid from header in message with queue_id: [{0}]".format(
            metadata.get_queue_id())
        )

    # parseaddr always result into 2 tuple ['username', 'email address']
    # if parse fails then result will be ['','']
    parsed_from_address = parseaddr(from_header.strip().replace(',', ''))

    if parsed_from_address is None or len(parsed_from_address) != 2:
        raise Exception("Invalid parsed_from_address tuple: [{0}]".format(parsed_from_address))

    from_sender = parsed_from_address[1]
    logger.debug("Parsed from header sender: [{0}]".format(from_sender))

    if from_sender is None or not from_sender:
        raise Exception("From header cannot be null or empty in outbound")

    return Metadata(
        metadata.get_schema_version(),
        metadata.get_sender_ip(),
        from_sender,
        metadata.get_accepting_server_ip(),
        metadata.get_queue_id(),
        metadata.get_date_recorded(),
        metadata.get_recipient_domain(),
        metadata.get_recipients(),
        metadata.get_x_sophos_email_id()
    )

# if get_metadata_for_outbound method finds x-sophos-effective-sender then this method returns the metadata
def get_metadata_for_outbound_effective_sender(metadata, header, header_value):

    header_sender = get_address_from_header(header_value)
    is_valid_sender = is_valid_outbound_sender(header_sender, header)

    if is_valid_sender:
        return Metadata(
            metadata.get_schema_version(),
            metadata.get_sender_ip(),
            header_sender,
            metadata.get_accepting_server_ip(),
            metadata.get_queue_id(),
            metadata.get_date_recorded(),
            metadata.get_recipient_domain(),
            metadata.get_recipients(),
            metadata.get_x_sophos_email_id()
        )

    raise Exception(
        "No header sender address could be validated for message with queue id [{0}]".format(
        metadata.get_queue_id()
        )
    )

def is_valid_outbound_sender(header_sender, header_name):

    if header_sender is None or not header_sender:
      raise Exception("Header {0} cannot be null or empty in outbound".format(header_name))

    header_sender_policy_exists = multipolicyreaderutils.policy_file_exists_in_S3(
          header_sender.lower(),
          AWS_REGION, POLICY_S3_BUCKET_NAME
        )

    return header_sender_policy_exists

# Given an email address header value, such as:
#
# "First Last" <first.last@somewhere.com>
#
# Uses the Regex pattern we use in Jilter to match and return the email address portion of the header
def get_address_from_header(header_string):
    parsed_address = None

    match_object =  re.search(EMAIL_ADDRESS_REGEX_PATTERN, header_string)

    if match_object is None or not match_object:
        return parsed_address

    parsed_address = match_object.group()
    return parsed_address

def get_from_sender_domain(metadata):
    try:
        sender = metadata.get_sender_address()
        queue_id = metadata.get_queue_id()
        if sender is None or not sender:
            raise Exception("Invalid from header sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        domain = sender.split('@')[1]
        matched_string = re.match(DOMAIN_NAME_REGEX_PATTERN, domain)

        if matched_string is None or not matched_string:
            raise Exception("Invalid from sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        return domain
    except:
        raise Exception("Error in retrieving domain from from sender [{0}] for queue_id [{1}]".format(
            sender, queue_id)
        )


def upload_documents(metadata, compressed_message, s3_file_path, direction, rfx_recovered):

    # prepared expiration date based on ttl_in_days
    expires = datetime.now() + timedelta(days=TTL_IN_DAYS)

    upload_message_to_s3(
        s3_file_path,
        expires,
        compressed_message,
        direction
    )

    upload_metadata_to_s3(
        s3_file_path,
        expires,
        metadata,
        direction
    )

    if should_upload_msg_history(direction, rfx_recovered):
        upload_msg_history_to_s3(
            s3_file_path,
            expires,
            metadata
        )


def should_upload_msg_history(direction, rfx_recovered):
    #We want MH update for recovered mail.
    if rfx_recovered is True:
        return True
    if INBOUND_MESSAGE_DIRECTION == direction:
        return True
    else:
        return False


def get_submit_message_type_for_encryption(direction):
    if direction == OUTBOUND_MESSAGE_DIRECTION:
        return SQS_MESSAGE_SUBMIT_TYPE_OUTBOUND
    else:
        return SQS_MESSAGE_SUBMIT_TYPE_INBOUND


def base_policy_flow(metadata, direction, is_reply, domain, sender, message, rfx_submit):
    queue_id = metadata.get_queue_id()

    tmp_file_path = formatterutils.get_s3_path(
        ROOT_DIR,
        TIMESTAMP_FORMAT,
        SUBMIT_HOST_IP,
        queue_id,
        domain
    )

    if (direction == OUTBOUND_MESSAGE_DIRECTION and is_reply == True) or direction == INBOUND_MESSAGE_DIRECTION \
        or rfx_submit is True:
        s3_file_path = tmp_file_path
    else:
        s3_file_path = formatterutils.get_s3_file_path(tmp_file_path, '-ENCR')

    sqs_message_submit_type = get_submit_message_type_for_encryption(direction)

    # create a sqs object
    sqs_message = SqsMessage(
        messageformatter.SCHEMA_VERSION,
        s3_file_path,
        SUBMIT_HOST_IP,
        queue_id,
        AKM_KEY,
        NONCE.encode('base64','strict'),
        MESSAGE_KEY.encode('base64','strict'),
        sqs_message_submit_type
    )

    upload_documents(
        metadata,
        gziputils.gzip_data(message),
        s3_file_path,
        direction,
        rfx_submit
    )

    send_msg_processing_sqs_message(
        sqs_message.get_sqs_json(),
        direction
    )

    if should_upload_msg_history(direction, rfx_submit):

        history_event = prepare_msg_history_event(
            s3_file_path,
            metadata,
            sqs_message,
            direction,
            sender
        )

        publish_msg_history_event(
            history_event.get_sqs_json()
        )


def get_direction(message):
    direction = OUTBOUND_MESSAGE_DIRECTION
    is_reply = False
    is_mh_parent = False

    parser = email.parser.HeaderParser()
    headers = parser.parsestr(message)

    for header, value in headers.items():
        if header == DELIVER_INBOUND_HEADER and value == DELIVER_INBOUND_VALUE:
            direction = INBOUND_MESSAGE_DIRECTION
        elif header == ECHOWORX_REPLY_HEADER:
            is_reply = True
        elif header == SOPHOS_PARENT_ID_HEADER:
            is_mh_parent = True

    if is_reply == True and is_mh_parent == True:
        is_reply = False

    return (direction, is_reply)

def get_message_headers(message):
    return Parser().parsestr(message, headersonly=True)

# We receive email from Reflexion recovery of a archived mail as well as encrypted mails from Echoworx here.
# Method accepts data from postfix and send message and metadata to S3 and
# a json object to SQS which provides pointer to the message in S3
def main():
    try:
        if 'ENCRYPTION' != SUBMIT_TYPE:
          raise ("Wrong submit type [{0}]".format(SUBMIT_TYPE))

        domain = None
        plaintext_message = get_plaintext_message()
        message_headers = get_message_headers(plaintext_message)
        metadata = get_metadata(message_headers)

        if rfxrecoveryutils.is_reflexion_ip(metadata.get_sender_ip()):
            rfx_submit = True
            logger.info(
                "Reflexion IP: [{0}] found from metdata treating as recovered or new mail from reflexion web UI".format(
                    metadata.get_sender_ip()))
            direction = rfxrecoveryutils.get_direction_for_reflexion_mail(message_headers)
            is_reply = False # Setting it false to avoid -ENCR append on path on reflexion mails
        else:
            rfx_submit = False
            direction, is_reply = get_direction(plaintext_message)

        sender = metadata.get_sender_address()
        if direction == OUTBOUND_MESSAGE_DIRECTION:
          metadata = get_metadata_for_outbound(metadata, message_headers)
          domain = get_from_sender_domain(metadata)
        else:
          domain = metadata.get_recipient_domain()

        # required logging for XgemailTestBase automation test
        logger.info("Metadata json info [{0}]".format(metadata))

        base_policy_flow(metadata, direction, is_reply, domain, sender, plaintext_message, rfx_submit)

    except BaseException as e:
        logger.exception("Failed in processing email [{0}]".format(e))
        exit(EX_TEMPFAIL)
    finally:
        # disable any previously set alarm
        signal.alarm(0)

# Method is called when an alarm goes off. If that happens, then
# we temporarily fail processing which in turn will make Postfix
# queue the email for a later delivery attempt.
def signal_handler(signum, frame):
    logger.warning('Handling signal <{0}>. Processing took longer than {1}s to complete'.format(signum, PROCESS_TIMEOUT_SECONDS))
    exit(EX_TEMPFAIL)

if __name__ == "__main__":
    # The signal library is used to force a timeout on this producer script if needed.
    # See https://docs.python.org/2/library/signal.html for more information.
    # A timed out process will return the proper EX_TEMPFAIL back to Postfix
    # which will then properly retry the message at a later time.
    signal.signal(signal.SIGALRM, signal_handler)

    # set an alarm to go off after PROCESS_TIMEOUT_SECONDS
    signal.alarm(PROCESS_TIMEOUT_SECONDS)

    main()
