#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4
#
# This script provides a communication between Postfix and S3. It works as a local
# delivery agent to submit server which accepts an email from postfix and upload it
# to a designated S3 bucket. In case of a failure email stays in postfix deferred
# queue and postfix retries later.
#
# Copyright: Copyright (c) 1997-2018. All rights reserved.
# Company: Sophos Limited or one of its affiliates.

import sys
sys.path.append("<%= @xgemail_utils_path %>")

import formatterutils
import gziputils
import io
import json
import logging
import messageformatter
import metadataformatter
import multipolicyreaderutils
import os
import re
import signal
import email
import uuidutils
import messagehistory
import mailinfoformatter
import rfxrecoveryutils
from email.parser import HeaderParser
from awshandler import AwsHandler
from common.metadata import Metadata
from common.sqsmessage import SqsMessage
from datetime import datetime
from datetime import timedelta
from email.parser import Parser
from email.utils import parseaddr
from logging.handlers import SysLogHandler

# Constants
AWS_REGION = "<%= @sqs_msg_producer_aws_region %>"
MSG_HISTORY_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_s3_bucket_name %>"
MSG_HISTORY_MS_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_ms_s3_bucket_name %>"
MSG_HISTORY_EVENTS_TOPIC_ARN = "<%= @sns_msg_history_events_sns_topic_arn %>"
PROCESS_TIMEOUT_SECONDS = <%= @sqs_msg_producer_process_timeout_seconds %>

SUBMIT_TYPE = "<%= @xgemail_submit_type %>"

S3_ENCRYPTION_ALGORITHM = "<%= @s3_encryption_algorithm %>"
SUBMIT_BUCKET_NAME = "<%= @sqs_msg_producer_s3_bucket_name %>"
CUSTOMER_SUBMIT_BUCKET_NAME = "<%= @sqs_msg_producer_s3_customer_submit_bucket_name %>"
POLICY_S3_BUCKET_NAME = "<%= @sqs_msg_producer_policy_s3_bucket_name %>"

SUBMIT_HOST_IP = "<%= @sqs_msg_producer_submit_ip %>"
SUBMIT_SQS_URL = "<%= @sqs_msg_producer_sqs_url %>"
CUSTOMER_SUBMIT_SQS_URL = "<%= @sqs_msg_producer_customer_submit_sqs_url %>"

TTL_IN_DAYS = <%= @sqs_msg_producer_ttl_in_days %>

MSG_HISTORY_V2_BUCKET = "<%= @msg_history_v2_bucket_name %>"
MSG_HISTORY_MAIL_INFO_MAX_BYTES_IN_MSG_CONTEXT = 20 * 1024
MSG_HISTORY_EVENT_DIR = "<%= @msg_history_event_dir %>"
MSG_HISTORY_EVENT_PROCESSOR_URL = "http://127.0.0.1:<%= @msg_history_event_processor_port %>/accept_event"

# Exit codes from sysexits
EX_TEMPFAIL = <%= @sqs_msg_producer_ex_temp_failure_code %>

#Message and metadata file details
BUFFER_SIZE = <%= @sqs_msg_producer_buffer_size %>
ROOT_DIR = "<%= @sqs_msg_producer_email_root_dir %>"
TIMESTAMP_FORMAT = "%Y/%m/%d/%H/%M"

# logging to syslog setup
logging.getLogger("botocore").setLevel(logging.WARNING)
logger = logging.getLogger('sqsmsgproducer')
logger.setLevel(logging.INFO)
handler = logging.handlers.SysLogHandler('/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(levelname)s %(message)s'
)
handler.formatter = formatter
logger.addHandler(handler)

# Encryption constants
# TODO: Need to be changed when Encryption is in place
AKM_KEY = "akm_key"
MESSAGE_KEY = "message_key"
NONCE = "nonce"
DOMAIN_NAME_REGEX_PATTERN = "[a-zA-Z0-9][-a-zA-Z0-9]*(\\.[-a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}"
EMAIL_ADDRESS_REGEX_PATTERN = "(([^<>()\[\]\\.,;:\s@\"]+(\.[^<>()\[\]\\.,;:\s@\"]+)*)|(\".+\"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))";

# Other constants
DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
MESSAGE_HISTORY_UNKNOWN_DESIGNATION = "UNKNOWN"
MESSAGE_HISTORY_ACCEPTED_EVENT = "ACCEPTED"
INBOUND_MESSAGE_DIRECTION = "INBOUND"
OUTBOUND_MESSAGE_DIRECTION = "OUTBOUND"

# Submit message types to be transmitted to the appropriate PIC for encrypted messages
SQS_MESSAGE_SUBMIT_TYPE_INBOUND = "INTERNET"
SQS_MESSAGE_SUBMIT_TYPE_OUTBOUND = "CUSTOMER"

# Definition of the header "X-Sophos-Deliver-Inbound: true" for the email package
DELIVER_INBOUND_HEADER = "X-Sophos-Deliver-Inbound"
DELIVER_INBOUND_VALUE = "true"
SOPHOS_PARENT_ID_HEADER = "X-Sophos-MH-Parent-Id"
ECHOWORX_REPLY_HEADER = "x-echoworx-portal"

#EmailProductType
GATEWAY = "Gateway"

awshandler = AwsHandler(AWS_REGION)

#Strip the dot at the end of sender and/or recipients addresses as rfc:1034 mentions to consider ending dot optional
def strip_dot_at_end(email_address):
  if email_address.endswith("."):
    return email_address[:-1]
  else:
    return email_address

#postfix pipe sends metadata as sysargs.
def get_metadata(message_headers):
    try:
        metadata_length = len(sys.argv)
        if (metadata_length < 7):
            logger.info(" Usage: xgemail_sqs_message_producer.py <null_sender> "+
                         "<sender> <client_address> <queue_id> <domain> <original_recipient> ")
            exit(EX_TEMPFAIL)

        null_sender = sys.argv[1]
        sender_address = strip_dot_at_end(sys.argv[2])
        sender_ip = sys.argv[3]
        queue_id = sys.argv[4]
        recipient_domain = sys.argv[5]
        # TODO: see if we can pipe arrival date from postfix
        date_recorded = datetime.utcnow().strftime(DATETIME_FORMAT)
        recipient_list = [ ]
        for i in range (6,len(sys.argv)):
            recipient_list.append(strip_dot_at_end(sys.argv[i]))

        if (sender_address == null_sender):
            sender_address = None

        x_sophos_header = uuidutils.get_x_sophos_email_id(message_headers['X-Sophos-Email-ID'], queue_id)
        x_sophos_email_product_type = message_headers['X-Sophos-Xgemail-Product-Type']

        metadata = Metadata(metadataformatter.SCHEMA_VERSION,
                            sender_ip,
                            sender_address,
                            SUBMIT_HOST_IP,
                            queue_id,
                            date_recorded,
                            recipient_domain,
                            recipient_list,
                            x_sophos_header,
                            False,
                            x_sophos_email_product_type)

        # required logging for EncryptionServiceTest automation test
        logger.info("Input email metadata info [{0}]".format(metadata))
        return metadata
    except Exception as e:
        logger.exception("Failed in preparing metadata [{0}]".format(e))
        exit(EX_TEMPFAIL)

def add_to_sqs(sqs_url, sqs_json):
    return awshandler.add_to_sqs(
        sqs_url,
        json.dumps(sqs_json)
    )

def upload_to_s3(bucket_name, file_path, formatted_data, expires):
    return awshandler.upload_data_in_s3(
        bucket_name,
        file_path,
        formatted_data,
        expires,
        S3_ENCRYPTION_ALGORITHM
    )

# uploads message file to S3
def upload_message_to_s3(s3_file_path, expires, compressed_message, direction):
    try:
        message_file_path = messageformatter.get_s3_message_path(
            s3_file_path
        )

        # required logging for XgemailTestBase automation test
        logger.info("Processing message [{0}]".format(message_file_path))

        #TODO: Encrypt mime message bytes after V1

        formatted_email_data = messageformatter.get_formatted_email_data(
            compressed_message
        )

        # upload message to the submit bucket
        upload_to_s3(
            get_submit_bucket_name(direction),
            message_file_path,
            formatted_email_data,
            expires
        )

        logger.debug("Uploaded message to S3 [{0}]".format(message_file_path))

    except Exception as e:
        logger.exception("Failed in uploading message to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


def upload_metadata_to_s3(s3_file_path, expires, metadata, direction):
    try:
        metadata_file_path = metadataformatter.get_s3_metadata_path(
            s3_file_path
        )
        logger.debug("Processing metadata [{0}]".format(metadata_file_path))

        #TODO: Encrypt metadata object after V1

        # metadata_magic_bytes, schema_version, nonce_length, gzip_metadata_json):
        formatted_metadata = metadataformatter.get_formatted_metadata(
            gziputils.gzip_data(
                json.dumps(metadata.get_metadata_json())
            )
        )

        # upload metadata to submit bucket
        upload_to_s3(
            get_submit_bucket_name(direction),
            metadata_file_path,
            formatted_metadata,
            expires
        )

        logger.debug("Uploaded metadata to S3 [{0}]".format(metadata_file_path))

    except Exception as e:
        logger.exception("Failed in uploading metadata to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)

# Retrieves the S3 bucket depending on the direction (inbound or outbound).
# This logic is used on the encryption-submit instance only and there are two
# S3 buckets defined on this instance. For the rest instances there is no direction
# switching, so the SUBMIT_BUCKET_NAME is used only, which has the specific value
# for the particular instance.
def get_submit_bucket_name(direction):
    if direction != INBOUND_MESSAGE_DIRECTION:
      return CUSTOMER_SUBMIT_BUCKET_NAME
    else:
      return SUBMIT_BUCKET_NAME


# sends msg processing sqs messages to sqs
def send_msg_processing_sqs_message(sqs_message_json, direction, customer_id = None):

    receiving_queue = get_sqs_url(direction)

    try:
        logger.debug("Processing SQS job [{0}] in queue [{1}]".format(sqs_message_json, receiving_queue))
        add_to_sqs(receiving_queue, sqs_message_json)
    except Exception as e:
        logger.exception("Failed in uploading message processing SQS job [{0}]".format(e))
        exit(EX_TEMPFAIL)

# Retrieves the SQS URL depending on the direction (inbound or outbound).
# This logic is used on the encryption-submit instance only and there are two
# SQS URLs defined on this instance. For the rest instances there is no direction
# switching, so the SUBMIT_SQS_URL is used only, which has the specific value
# for the particular instance.
def get_sqs_url(direction):
    if direction != INBOUND_MESSAGE_DIRECTION:
      return CUSTOMER_SUBMIT_SQS_URL
    else:
      return SUBMIT_SQS_URL

def get_plaintext_message():
    stdin_no = sys.stdin.fileno()
    plaintext_email_bytes = io.BytesIO()

    try:
        while True:
            try:
                new_bytes = os.read(stdin_no, BUFFER_SIZE)
                if not new_bytes:
                    break
                plaintext_email_bytes.write(new_bytes)
            except EOFError:
                break
            except Exception as e:
                logger.exception("Failed in parsing input email [{0}]".format(e))
                exit(EX_TEMPFAIL)
            except BaseException as e:
                logger.exception("Unexpected error in parsing input email: [{0}]".format(e))
                exit(EX_TEMPFAIL)
        return plaintext_email_bytes.getvalue()
    finally:
        plaintext_email_bytes.close()

def get_metadata_for_outbound(metadata, message_headers, msghistory_events):
    effective_sender_value = message_headers['x-sophos-effective-sender']
    effective_sender_header = 'x-sophos-effective-sender'

    if effective_sender_value is not None:
        return get_metadata_for_outbound_effective_sender(metadata, effective_sender_header, effective_sender_value, msghistory_events)

    from_header = message_headers['from']
    if from_header is None:
        raise Exception("Invalid from header in message with queue_id: [{0}]".format(
            metadata.get_queue_id())
        )

    # parseaddr always result into 2 tuple ['username', 'email address']
    # if parse fails then result will be ['','']
    parsed_from_address = parseaddr(from_header.strip().replace(',', ''))

    if parsed_from_address is None or len(parsed_from_address) != 2:
        raise Exception("Invalid parsed_from_address tuple: [{0}]".format(parsed_from_address))

    from_sender = parsed_from_address[1]
    logger.debug("Parsed from header sender: [{0}]".format(from_sender))

    if from_sender is None or not from_sender:
        raise Exception("From header cannot be null or empty in outbound")

    return Metadata(
        metadata.get_schema_version(),
        metadata.get_sender_ip(),
        from_sender,
        metadata.get_accepting_server_ip(),
        metadata.get_queue_id(),
        metadata.get_date_recorded(),
        metadata.get_recipient_domain(),
        metadata.get_recipients(),
        metadata.get_x_sophos_email_id(),
        False,
        metadata.get_email_product_type()
    )

def get_customer_id(queue_id, msghistory_events):
    try:
        if msghistory_events is not None and len(msghistory_events) > 0:
            accepted_event = msghistory_events.itervalues().next()
            return accepted_event['mail_info']['customer_id']
    except Exception as ex:
        logger.warning("Queue Id [{0}]. Error in getting customer id [{1}]".format(queue_id, ex))

# if get_metadata_for_outbound method finds x-sophos-effective-sender then this method returns the metadata
def get_metadata_for_outbound_effective_sender(metadata, header, header_value, msghistory_events):

    header_sender = get_address_from_header(header_value)
    customer_id = get_customer_id(metadata.get_queue_id(), msghistory_events)
    is_valid_sender = False
    if multipolicyreaderutils.get_valid_sender_from_msghistory_enabled(customer_id, SUBMIT_HOST_IP) and msghistory_events is not None and len(msghistory_events) > 0:
        is_valid_sender = (header_value == list(msghistory_events.keys())[0])
    else:
        is_valid_sender = is_valid_outbound_sender(header_sender, header)

    if is_valid_sender:
        return Metadata(
            metadata.get_schema_version(),
            metadata.get_sender_ip(),
            header_sender,
            metadata.get_accepting_server_ip(),
            metadata.get_queue_id(),
            metadata.get_date_recorded(),
            metadata.get_recipient_domain(),
            metadata.get_recipients(),
            metadata.get_x_sophos_email_id(),
            False,
            metadata.get_email_product_type()
        )

    raise Exception(
        "No header sender address could be validated for message with queue id [{0}]".format(
        metadata.get_queue_id()
        )
    )

def is_valid_outbound_sender(header_sender, header_name):

    if header_sender is None or not header_sender:
      raise Exception("Header {0} cannot be null or empty in outbound".format(header_name))

    header_sender_policy_exists = multipolicyreaderutils.policy_file_exists_in_S3(
          header_sender.lower(),
          AWS_REGION, POLICY_S3_BUCKET_NAME
        )

    return header_sender_policy_exists

# Given an email address header value, such as:
#
# "First Last" <first.last@somewhere.com>
#
# Uses the Regex pattern we use in Jilter to match and return the email address portion of the header
def get_address_from_header(header_string):
    parsed_address = None

    match_object =  re.search(EMAIL_ADDRESS_REGEX_PATTERN, header_string)

    if match_object is None or not match_object:
        return parsed_address

    parsed_address = match_object.group()
    return parsed_address

def get_from_sender_domain(metadata):
    try:
        sender = metadata.get_sender_address()
        queue_id = metadata.get_queue_id()
        if sender is None or not sender:
            raise Exception("Invalid from header sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        domain = sender.split('@')[1]
        matched_string = re.match(DOMAIN_NAME_REGEX_PATTERN, domain)

        if matched_string is None or not matched_string:
            raise Exception("Invalid from sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        return domain
    except:
        raise Exception("Error in retrieving domain from from sender [{0}] for queue_id [{1}]".format(
            sender, queue_id)
        )


def upload_documents(metadata, compressed_message, s3_file_path, direction, rfx_recovered):

    # prepared expiration date based on ttl_in_days
    expires = datetime.now() + timedelta(days=TTL_IN_DAYS)

    upload_message_to_s3(
        s3_file_path,
        expires,
        compressed_message,
        direction
    )

    upload_metadata_to_s3(
        s3_file_path,
        expires,
        metadata,
        direction
    )




def should_upload_msg_history(direction, rfx_recovered):
    #We want MH update for recovered mail.
    if rfx_recovered is True:
        return True
    if INBOUND_MESSAGE_DIRECTION == direction:
        return True
    else:
        return False


def get_submit_message_type_for_encryption(direction):
    if direction == OUTBOUND_MESSAGE_DIRECTION:
        return SQS_MESSAGE_SUBMIT_TYPE_OUTBOUND
    else:
        return SQS_MESSAGE_SUBMIT_TYPE_INBOUND

def upload_msghistory_mail_info_to_s3(s3_file_path, msghistory_mail_info):
    try:
        msghistory_mail_info_path = mailinfoformatter.get_mail_info_path(
            s3_file_path
        )
        logger.debug("Processing message history [{0}]".format(msghistory_mail_info_path))

        formatted_mail_info = mailinfoformatter.get_formatted_mail_info(
            gziputils.gzip_data(
                json.dumps(msghistory_mail_info)
            ),
            msghistory_mail_info['schema_version']
        )

        # prepared expiration date based on ttl_in_days
        expires = datetime.now() + timedelta(days=TTL_IN_DAYS)

        # upload msg history to message history bucket
        upload_to_s3(
            MSG_HISTORY_V2_BUCKET,
            msghistory_mail_info_path,
            formatted_mail_info,
            expires
        )

        logger.debug("Uploaded mail info to S3 [{0}]".format(msghistory_mail_info_path))
        return msghistory_mail_info_path
    except Exception as e:
        logger.warning("S3 File Path [{0}]. Failed in uploading mail info to S3 [{1}]".format(s3_file_path, e))

def prepare_message_context(s3_file_path, mail_info):

    message_context = None

    #convert mail_info to str
    mail_info_str = json.dumps(mail_info)

    if len(mail_info_str) > MSG_HISTORY_MAIL_INFO_MAX_BYTES_IN_MSG_CONTEXT:
        mail_info_path = upload_msghistory_mail_info_to_s3(s3_file_path, mail_info)
        if mail_info_path is not None:
            message_context = {'mh_context' : { 'mail_info_s3_path' :  mail_info_path } }
    else:
        message_context = { 'mh_context' : { 'mail_info' : mail_info } }
    return message_context

#The accepted events written by Jilter needs to be updated with 's3_file_path' and 'decorated_queue_id'
def update_msghistory_event_and_get_message_context(msghistory_events, s3_file_path, policy_metadata, direction,
                                                    recipients, sender, upload_msg_history, is_mh_parent):
    try:
        mail_info = None

        messagehistory.update_msghistory_event(msghistory_events, s3_file_path, policy_metadata, direction, recipients,
                                               sender, policy_metadata.get_email_product_type())
        mailbox = None
        if direction == INBOUND_MESSAGE_DIRECTION:
            mailbox = recipients[0].lower()
        else:
            mailbox = sender.lower()
        if (mailbox in msghistory_events and
            'mail_info' in msghistory_events[mailbox]):
            mail_info = msghistory_events[mailbox]['mail_info']
            #if 'is_mh_parent' is present, it is the actual encrypted mail
            #coming from echoworkz. We need to generate delivery events.
            #so not setting 'generate_mh_events' to failse if 'is_mh_parent' is true.
            if not upload_msg_history and not is_mh_parent:
                mail_info['generate_mh_events'] = False
            return prepare_message_context(s3_file_path, mail_info)
    except Exception as ex:
        logger.warning("Queue Id [{0}]. Error in processing message history event [{1}]".format(policy_metadata.get_queue_id(), ex))

def get_s3_prefix_path(queue_id, mailbox_id, domain):
    root_dir = None

    if get_email_product_type() == MAILFLOW:
        root_dir = MAIL_FLOW_ROOT_DIR
    else:
        root_dir = ROOT_DIR

    s3_file_path = formatterutils.get_s3_prefix_file_path(
        root_dir,
        queue_id,
        mailbox_id,
        SUBMIT_HOST_IP,
        domain
    )

    return s3_file_path

def update_policy_context(message_context, recipient, policy):
    if policy is None or recipient is None or policy.get(recipient.lower()) is None:
        return message_context
    message_context.update({"policy":policy.get(recipient.lower())})
    return message_context


def base_policy_flow(metadata, direction, is_reply, domain, sender, message, rfx_submit, is_mh_parent, msghistory_events, policy):
    queue_id = metadata.get_queue_id()

    tmp_file_path = formatterutils.get_s3_path(
        ROOT_DIR,
        TIMESTAMP_FORMAT,
        SUBMIT_HOST_IP,
        queue_id,
        domain
    )

    if multipolicyreaderutils.prefix_messages_path_enabled(get_customer_id(queue_id, msghistory_events), SUBMIT_HOST_IP):
        tmp_file_path = get_s3_prefix_path(queue_id, policy_id, domain)

    if (direction == OUTBOUND_MESSAGE_DIRECTION and is_reply == True) or direction == INBOUND_MESSAGE_DIRECTION \
        or rfx_submit is True:
        s3_file_path = tmp_file_path
    else:
        s3_file_path = formatterutils.get_s3_file_path(tmp_file_path, '-ENCR')

    sqs_message_submit_type = get_submit_message_type_for_encryption(direction)

    upload_msg_history = should_upload_msg_history(direction, rfx_submit)

    message_context = None
    if msghistory_events is not None:
      message_context = update_msghistory_event_and_get_message_context(msghistory_events, s3_file_path, metadata, direction, metadata.get_recipients(), sender, upload_msg_history, is_mh_parent)

    #Check is policy is not empty and number of recipient is more than 1 for context
    if policy is not None and len(metadata.get_recipients()) >= 1:
        message_context = update_policy_context(message_context, metadata.get_recipients()[0], policy)

    # create a sqs object
    sqs_message = SqsMessage(
        messageformatter.SCHEMA_VERSION,
        s3_file_path,
        SUBMIT_HOST_IP,
        queue_id,
        AKM_KEY,
        NONCE.encode('base64','strict'),
        MESSAGE_KEY.encode('base64','strict'),
        sqs_message_submit_type,
        None,
        None,
        message_context
    )

    upload_documents(
        metadata,
        gziputils.gzip_data(message),
        s3_file_path,
        direction,
        rfx_submit
    )

    send_msg_processing_sqs_message(
        sqs_message.get_sqs_json(),
        direction
    )

    if upload_msg_history:
        if msghistory_events is not None:
            messagehistory.handle_msghistory_events(queue_id, msghistory_events, MSG_HISTORY_EVENT_PROCESSOR_URL, MSG_HISTORY_EVENT_DIR)
        else:
            logger.warning("MH events not generated for Queue ID [{0}]".format(queue_id))


def get_direction(message):
    direction = OUTBOUND_MESSAGE_DIRECTION
    is_reply = False
    is_mh_parent = False

    parser = email.parser.HeaderParser()
    headers = parser.parsestr(message)

    for header, value in headers.items():
        if header == DELIVER_INBOUND_HEADER and value == DELIVER_INBOUND_VALUE:
            direction = INBOUND_MESSAGE_DIRECTION
        elif header == ECHOWORX_REPLY_HEADER:
            is_reply = True
        elif header == SOPHOS_PARENT_ID_HEADER:
            is_mh_parent = True

    if is_reply == True and is_mh_parent == True:
        is_reply = False

    return (direction, is_reply, is_mh_parent)

def get_message_headers(message):
    return Parser().parsestr(message, headersonly=True)

# We receive email from Reflexion recovery of a archived mail as well as encrypted mails from Echoworx here.
# Method accepts data from postfix and send message and metadata to S3 and
# a json object to SQS which provides pointer to the message in S3
def main():
    try:
        if 'ENCRYPTION' != SUBMIT_TYPE:
          raise ("Wrong submit type [{0}]".format(SUBMIT_TYPE))

        domain = None
        plaintext_message = get_plaintext_message()
        message_headers = get_message_headers(plaintext_message)
        metadata = get_metadata(message_headers)
        queue_id = metadata.get_queue_id()
        #A dict with msghistory_events and policy
        jilter_context = messagehistory.read_jilter_context(queue_id, MSG_HISTORY_EVENT_DIR)
        msghistory_events = {}
        policy = None
        if jilter_context is not None and jilter_context.get('msghistory_events') is not None:
            msghistory_events=jilter_context.get('msghistory_events')
        if jilter_context is not None and jilter_context.get('policy') is not None and len(policy) > 0:
            policy = jilter_context.get('policy')

        if rfxrecoveryutils.is_reflexion_ip(metadata.get_sender_ip()):
            rfx_submit = True
            logger.info(
                "Reflexion IP: [{0}] found from metdata treating as recovered or new mail from reflexion web UI".format(
                    metadata.get_sender_ip()))
            direction = rfxrecoveryutils.get_direction_for_reflexion_mail(message_headers)
            is_reply = False # Setting it false to avoid -ENCR append on path on reflexion mails
            is_mh_parent = False
        else:
            rfx_submit = False
            direction, is_reply, is_mh_parent = get_direction(plaintext_message)

        sender = metadata.get_sender_address()
        if direction == OUTBOUND_MESSAGE_DIRECTION:
          metadata = get_metadata_for_outbound(metadata, message_headers, msghistory_events)
          domain = get_from_sender_domain(metadata)
        else:
          domain = metadata.get_recipient_domain()

        # required logging for XgemailTestBase automation test
        logger.info("Metadata json info [{0}]".format(metadata))

        base_policy_flow(metadata, direction, is_reply, domain, sender, plaintext_message, rfx_submit, is_mh_parent, msghistory_events, policy)

    except BaseException as e:
        logger.exception("Failed in processing email [{0}]".format(e))
        exit(EX_TEMPFAIL)
    finally:
        # disable any previously set alarm
        signal.alarm(0)

# Method is called when an alarm goes off. If that happens, then
# we temporarily fail processing which in turn will make Postfix
# queue the email for a later delivery attempt.
def signal_handler(signum, frame):
    logger.warning('Handling signal <{0}>. Processing took longer than {1}s to complete'.format(signum, PROCESS_TIMEOUT_SECONDS))
    exit(EX_TEMPFAIL)

if __name__ == "__main__":
    # The signal library is used to force a timeout on this producer script if needed.
    # See https://docs.python.org/2/library/signal.html for more information.
    # A timed out process will return the proper EX_TEMPFAIL back to Postfix
    # which will then properly retry the message at a later time.
    signal.signal(signal.SIGALRM, signal_handler)

    # set an alarm to go off after PROCESS_TIMEOUT_SECONDS
    signal.alarm(PROCESS_TIMEOUT_SECONDS)

    main()