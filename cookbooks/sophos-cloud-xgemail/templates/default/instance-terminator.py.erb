#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4

"""
Processes CloudWatch Event generated from Lifecycle Hook in order to perform an orderly shutdown of EC2 Instance.

Copyright 2018, Sophos Limited. All rights reserved.

'Sophos' and 'Sophos Anti-Virus' are registered trademarks of
Sophos Limited and Sophos Group.  All other product and company
names mentioned are trademarks or registered trademarks of their
respective owners.
"""

import argparse
import boto3
import botocore
import sys
import os
import json
import logging
import logging.handlers
import subprocess
from time import sleep
from datetime import datetime
from botocore.exceptions import ClientError, WaiterError


# logging to syslog setup
logger = logging.getLogger('instance-terminator')
logger.setLevel(logging.INFO)
syslog_handler = logging.handlers.SysLogHandler(address='/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(levelname)s %(message)s'
)
syslog_handler.formatter = formatter
logger.addHandler(syslog_handler)


class Terminator(object):

    def __init__(self, region, time, asg, instance, hook, token):

        self.logger = logging.getLogger('instance-terminator')
        self.postfix_instance = None
        self.aws_region = region
        self.timestamp = time
        self.delay = 5
        self.autoscaling_group_name = asg
        self.lifecycle_hook_name = hook
        self.lifecycle_action_token = token
        self.sns_alarm_topic_arn = "<%= @alarm_topic_arn %>"
        self.sqs_consumer_service = "<%= @sqs_consumer_service_name %>"
        self.sns_policy_arn = "<%= @sns_policy_arn %>"
        self.sqs_policy_queue_name = "<%= @sqs_policy_queue_name %>"
        #self.volume_cleanup = <%= @volume_cleanup %>
        self.postfix_queue_count = None
        self.volume_device_name = '/dev/xvdi'

        # Create EC2 Resource
        self.ec2 = boto3.resource('ec2', region_name=self.aws_region)
        """:type: pyboto3.ec2 """
        # Create EC2 Client
        self.ec2c = self.ec2.meta.client
        """:type: pyboto3.ec2 """

        # Create SQS Client
        self.sqs = boto3.resource('sqs', region_name=self.aws_region)
        """:type: pyboto3.sqs """
        self.sqs_client = self.sqs.meta.client
        """:type: pyboto3.sqs """

        # Create SNS Client
        self.sns = boto3.resource('sns', region_name=self.aws_region)
        """:type: pyboto3.sns """
        self.sns_client = self.sns.meta.client
        """:type: pyboto3.sns """

        # Create SSM Client
        self.ssm = boto3.client('ssm', region_name=self.aws_region)
        """:type: pyboto3.ssm """

        self.instance = self.ec2.Instance(instance)
        self.volume = self.ec2_get_volume()

    def ec2_get_volume(self):
        for v in self.instance.block_device_mappings:
            if v['DeviceName'] == self.volume_device_name:
                return self.ec2.Volume(v['Ebs']['VolumeId'])
        return None

    def ec2_detach_volume(self, force=False):
        """
       Detach Ebs Volume from EC2 Instance.
        """
        try:
            self.volume.detach_from_instance(
                Device=self.volume_device_name,
                Force=force,
                InstanceId=self.instance.id
            )
            self.ec2c.get_waiter('volume_available').wait(
                VolumeIds=[self.volume.id],
                WaiterConfig={
                    'Delay': self.delay
                }
            )
            self.logger.info("Volume Detached")
            return True
        except (ClientError, WaiterError) as e:
            self.logger.exception("Exception detaching Volume: {}".format(e.response['Error']['Code']))
            return False

    def ec2_delete_volume(self):
        """
       Delete Ebs Volume.
        """
        self.volume.delete()
        self.ec2c.get_waiter('volume_deleted').wait(
            VolumeIds=[self.volume.id],
            WaiterConfig={
                'Delay': self.delay
            }
        )
        logger.info("Volume Deleted")

    def get_postfix_instance(self):
        """
        Return the Postfix Multi-Instance name.
        """
        output = self.run_cmd(
            ['postmulti', '-l'],
            'Getting Postfix Instance'
        )
        for line in output.split('\n'):
            if 'mta' in line:
                return line.split()[0]

    def run_cmd(self, cmd, comment=None):
        """
        Run a provided shell command.
        """
        if not cmd:
            self.logger.error("Function: run_cmd - Command not provided.")
            sys.exit(1)

        if comment:
            self.logger.info(comment)

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )

        out, err = process.communicate()

        if process.returncode != 0:
            self.logger.error("Exception while running command [ {} ], exiting.".format(cmd))
            sys.exit(process.returncode)

        return out.strip()

    def send_sns_alert(self, message):
        """
        Send provided message as an alarm to specified Amazon Web Services SNS Topic.
        """
        if not message:
            self.logger.info("Function: send_sns_alert - Need a message to send an alert.")
            sys.exit(1)
        try:
            response = self.sns.publish(
                TopicArn=self.sns_alarm_topic_arn,
                Subject="Alarm from EC2 Instance: {} in Region: {}".format(self.instance.id, self.aws_region),
                Message=message
            )
            self.logger.info("SNS MessageId: {}".format(response['MessageId']))
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception sending SNS alert to alarm topic {} : {}".format(
                self.sns_alarm_topic_arn, e.response['Error']['Code']))

    def service_controller(self, service, action):
        """
        Used to control the Xgemail SQS Consumer Service.
        """
        process = subprocess.Popen(
            ('/sbin/service', service, action),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        out, err = process.communicate()
        self.logger.info(out.strip())

        return process.returncode

    def stop_sqs_consumer(self):
        """
        Stop SQS consumer (used on delivery instances)
        Returns True if the service is stopped or False if there is a problem
        Does it's best to detect problems and fix them.
        """
        if not self.sqs_consumer_service:
            self.logger.error("Function: stop_sqs_consumer - sqs_service not provided.")
            sys.exit(1)

        status = self.service_controller(self.sqs_consumer_service, 'status')
        if status == 0:
            self.logger.info("Xgemail SQS Consumer Service is running. Stopping Service.")
            if self.service_controller(self.sqs_consumer_service, 'stop') == 0:
                self.logger.info("Xgemail SQS Consumer Service is Stopped.")
                return True
        if (status == 1) or (status == 2):
            self.logger.warning("Xgemail SQS Consumer Service is dead but pid and lock files exist. Forcing a restart then stopping cleanly.")
            if self.service_controller(self.sqs_consumer_service, 'force-restart') == 0:
                self.logger.info("Xgemail SQS Consumer Service is Running. Stopping Service")
                if self.service_controller(self.sqs_consumer_service, 'stop') == 0:
                    self.logger.info("Xgemail SQS Consumer Service is Stopped.")
                    return True
        if status == 3:
            self.logger.info("Xgemail SQS Consumer Service is already Stopped.")
            return True

        error_message = "CRITICAL! cannot stop Xgemail SQS Consumer!"
        self.send_sns_alert(error_message)
        return False

    def check_postfix_queue(self):
        """
        Check the postfix queue.
        Returns the number of messages in the Postfix queue
        """
        subdirs = ['maildrop', 'incoming', 'active', 'deferred']
        messages = 0

        for q in subdirs:
            # TODO: replace static storage path with new asg volume when XGE-1807 is pulled in
            path = ("/storage/{}/{}".format(self.postfix_instance, q))
            for root, dirs, files in os.walk(path):
                messages += len(files)

        return messages

    def process_event(self):
        """
        Main function that processes the CloudWatch Event generated from the Lifecycle Hook.
        """

        self.logger.info("Begin processing Lifecycle Hook Event")
        self.service_controller('monit', 'stop')

        lifecycle_hook = LifecycleHook(region=self.aws_region, time=self.timestamp, asg=self.autoscaling_group_name, instance=self.instance.id, hook=self.lifecycle_hook_name, token=self.lifecycle_action_token)

        # Now perform all necessary logic to make sure the postfix server has empty queues before terminating.
        self.postfix_instance = self.get_postfix_instance()
        if self.postfix_instance == 'postfix-cd' or self.postfix_instance == 'postfix-id':
            if self.stop_sqs_consumer() is False:
                self.logger.error("Unable to continue. There is a problem with the Xgemail SQS Consumer Service.")
                sys.exit(1)

        self.postfix_queue_count = self.check_postfix_queue()
        self.logger.info("Messages in Postfix queue: %d" % self.postfix_queue_count)

        # If there are still messages in the Postfix queue
        while self.postfix_queue_count != 0:
            self.logger.warning("There are {} email messages still in the queue. We need to wait for them to drain.".format(self.postfix_queue_count))

            for lifecycle_hook.heartbeat_request_count in range(1, 100, 1):
                while lifecycle_hook.time_check():
                    last_count = self.postfix_queue_count
                    sleep(10)

                    self.postfix_queue_count = self.check_postfix_queue()
                    self.logger.info("Queue Count: %d" % self.postfix_queue_count)

                    if self.postfix_queue_count == 0:
                        break

                    elif self.postfix_queue_count < last_count:
                        continue

                    elif self.postfix_queue_count >= last_count:
                        self.logger.warning("Email queue doesn't seem to be draining. Flushing queue and waiting 30 seconds before checking again.")
                        # Attempt to flush postfix queue then wait for drop
                        self.run_cmd(['/usr/sbin/postmulti', '-i', self.postfix_instance, '-x', 'postqueue', '-f'])
                        sleep(30)

                        self.postfix_queue_count = self.check_postfix_queue()

                        if self.postfix_queue_count < last_count:
                            self.logger.info("Email queue is draining. Continuing with regular check.")
                            continue

                        elif self.postfix_queue_count >= last_count:
                            error_message = "CRITICAL! *** {} *** Undelivered messages are stuck in the email queue.".format(self.postfix_queue_count)
                            self.logger.critical(error_message)
                            #self.send_sns_alert(error_message)
                            return self.postfix_queue_count

                if self.postfix_queue_count == 0:
                    break

                self.logger.warning("Time Check - Time is running out!! Recording Lifecycle Action HeartBeat.")
                lifecycle_hook.record_lifecycle_action_heartbeat()

            if self.postfix_queue_count == 0:
                self.logger.info("The Postfix queue is empty.")
                #break
                return self.postfix_queue_count

            self.logger.critical("LIFECYCLE HOOK EXHAUSTED!")
            return self.postfix_queue_count

    def sqs_sns_cleanup(self):
        #Unsubscribe SQS Queue from SNS Topic
        for service in os.listdir('/etc/init.d/'):
            if service.startswith('xgemail-'):
                self.service_controller(service, 'stop')
        # Also Remove Policy SNS subscription and delete SQS queue for this instance
        try:
            sqs_policy_queue = self.sqs.get_queue_by_name(QueueName=self.sqs_policy_queue_name)
            sqs_policy_queue_dlq = self.sqs.get_queue_by_name(QueueName=self.sqs_policy_queue_name + '-DLQ')
            sns_policy_topic = self.sns.Topic(self.sns_policy_arn)
            self.logger.info('Getting policy subscription ARN.')
            sns_policy_subscription = [s for s in sns_policy_topic.subscriptions.all() if s.attributes['Endpoint'] == sqs_policy_queue.attributes['QueueArn']][0]
            self.logger.info("Unsubscribing SNS Subscription {}.".format(sns_policy_subscription.arn))
            sns_policy_subscription.delete()
            self.logger.info("Deleting SQS queue {}".format(sqs_policy_queue.url))
            sqs_policy_queue.delete()
            self.logger.info("Deleting SQS queue {}".format(sqs_policy_queue_dlq.url))
            sqs_policy_queue_dlq.delete()
        except ClientError as e:
            self.logger.exception("Exception during SQS and SNS cleanup: {}".format(e.response['Error']['Code']))
            pass

    def cleanup_volume(self):
        if self.postfix_queue_count == 0:
            self.logger.info("The Postfix queue is empty! Unmounting, detaching, and deleting volume.")
            # Unmount Volume
            self.run_cmd(['umount', self.volume_device_name])
            sleep(5)
            self.logger.info("Volume Unmounted")
            # Detach the volume
            self.logger.info("Detaching Ebs Volume: {} from EC2 Instance: {}.".format(self.volume.id, self.instance.id))
            self.ec2_detach_volume()
            self.volume.reload()
            if self.volume.state != 'available':
                self.logger.info("Volume is still not available. Force detaching.")
                self.ec2_detach_volume(force=True)
            # Delete the volume
            self.logger.info("Deleting Ebs Volume: {}.".format(self.volume.id))
            self.ec2_delete_volume()
        # Unsubscribe SQS Queue from SNS Topic
        # if self.postfix_instance == 'postfix-is' or self.postfix_instance == 'postfix-cs':
        #     for service in os.listdir('/etc/init.d/'):
        #         if service.startswith('xgemail-'):
        #             self.service_controller(service, 'stop')
        #     # Also Remove Policy SNS subscription and delete SQS queue for this instance
        #     try:
        #         sqs_policy_queue = self.sqs.get_queue_by_name(QueueName=self.sqs_policy_queue_name)
        #         sqs_policy_queue_dlq = self.sqs.get_queue_by_name(QueueName=self.sqs_policy_queue_name + '-DLQ')
        #         sns_policy_topic = self.sns.Topic(self.sns_policy_arn)
        #         self.logger.info('Getting policy subscription ARN.')
        #         sns_policy_subscription = [s for s in sns_policy_topic.subscriptions.all() if s.attributes['Endpoint'] == sqs_policy_queue.attributes['QueueArn']][0]
        #         self.logger.info("Unsubscribing SNS Subscription {}.".format(sns_policy_subscription.arn))
        #         sns_policy_subscription.delete()
        #         self.logger.info("Deleting SQS queue {}".format(sqs_policy_queue.url))
        #         sqs_policy_queue.delete()
        #         self.logger.info("Deleting SQS queue {}".format(sqs_policy_queue_dlq.url))
        #         sqs_policy_queue_dlq.delete()
        #     except ClientError as e:
        #         self.logger.exception("Exception during SQS and SNS cleanup: {}".format(e.response['Error']['Code']))
        #         pass
        #
        # if self.postfix_queue_count != 0:
        #     self.logger.critical("The Postfix queue is NOT empty! Unmounting and detaching volume for further investigation.")
        #
        # if self.volume_cleanup:
        #     if self.postfix_queue_count == 0:
        #         self.logger.info("The Postfix queue is empty! Unmounting, detaching, and deleting volume.")
        #         # Unmount Volume
        #         self.run_cmd(['umount', self.volume_device_name])
        #         sleep(5)
        #         self.logger.info("Volume Unmounted")
        #         # Detach the volume
        #         self.logger.info("Detaching Ebs Volume: {} from EC2 Instance: {}.".format(self.volume.id, self.instance.id))
        #         self.ec2_detach_volume()
        #         self.volume.reload()
        #         if self.volume.state != 'available':
        #             self.logger.info("Volume is still not available. Force detaching.")
        #             self.ec2_detach_volume(force=True)
        #         # Delete the volume
        #         self.logger.info("Deleting Ebs Volume: {}.".format(self.volume.id))
        #         self.ec2_delete_volume()
        # # Complete ASG hook
        # self.logger.info('Completing lifecycle action.')
        # lifecycle_hook.complete_lifecycle_action()
        print
        sys.exit(0)


class LifecycleHook(object):

    def __init__(self, region, time, asg, instance, hook, token):
        self.logger = logging.getLogger('instance-terminator')
        self.aws_region = region
        # Create AS client
        self.asc = boto3.client('autoscaling', region_name=self.aws_region)
        """:type: pyboto3.autoscaling """
        self.aws_time_format = '%Y-%m-%dT%H:%M:%SZ'
        self.autoscaling_group_name = asg
        self.lifecycle_hook_name = hook
        self.instance_id = instance
        self.lifecycle_global_timeout = self.get_timeout('GlobalTimeout')
        self.heartbeat_request_count = 1
        self.heartbeat_timeout = self.get_timeout('HeartbeatTimeout')
        self.heartbeat_start_time = datetime.strptime(time, self.aws_time_format)
        self.lifecycle_action_token = token

    def time_check(self):
        """
        Calculates elapsed time: Current time in AWS format '%Y-%m-%dT%H:%M:%S.%fZ' minus the Heartbeat Start Time in AWS format.
        """
        seconds_buffer = 60
        elapsed_time = datetime.strptime(datetime.now().strftime(self.aws_time_format), self.aws_time_format) - self.heartbeat_start_time
        elapsed_time.total_seconds()
        self.logger.info("Time Check - Elapsed Time: {:04.1f} seconds, Heartbeat Timeout: {} seconds, Heartbeat Request Count: {}".format(elapsed_time.total_seconds(), self.heartbeat_timeout, self.heartbeat_request_count))
        if elapsed_time.total_seconds() <= self.heartbeat_timeout - seconds_buffer:
            return True
        else:
            return False

    def get_timeout(self, timeout):
        """
        Takes timeout parameter (GlobalTimeout or HeartbeatTimeout) and returns associated value from Describe Lifecycle Hooks.
        """
        try:
            response = self.asc.describe_lifecycle_hooks(
                AutoScalingGroupName=self.autoscaling_group_name,
                LifecycleHookNames=[self.lifecycle_hook_name]
            )['LifecycleHooks'][0][timeout]
            self.logger.info("{}: {}".format(timeout, response))
            return response
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception getting timeout for LifecycleHook {}: {}".format(
                self.lifecycle_hook_name, e.response['Error']['Code']))

    def record_lifecycle_action_heartbeat(self):
        """
        Records a heartbeat for the lifecycle action associated with the specified token or instance.
        This extends the timeout by the length of time defined using PutLifecycleHook.
        Resets the Heartbeat Start Time.
        """
        try:
            response = self.asc.record_lifecycle_action_heartbeat(
                LifecycleHookName=self.lifecycle_hook_name,
                AutoScalingGroupName=self.autoscaling_group_name,
                LifecycleActionToken=self.lifecycle_action_token,
                InstanceId=self.instance_id
            )
            self.heartbeat_start_time = datetime.strptime(datetime.now().strftime(self.aws_time_format), self.aws_time_format)
            self.logger.info("Recording Lifecycle Action Heartbeat: {} and resetting heartbeat start time to ".format(response, self.heartbeat_start_time))
            return response
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception recording lifecycle action heartbeat for instance {}: Lifecycle Hook Name {}: Lifecycle Action Token {}: {}".format(
                self.instance_id, self.lifecycle_hook_name, self.lifecycle_action_token, e.response['Error']['Code']))

    def complete_lifecycle_action(self):
        """
        Completes the lifecycle action for the specified token or instance with the specified result.
        """
        try:
            self.asc.complete_lifecycle_action(
                LifecycleHookName=self.lifecycle_hook_name,
                AutoScalingGroupName=self.autoscaling_group_name,
                LifecycleActionToken=self.lifecycle_action_token,
                LifecycleActionResult='CONTINUE',
            )
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception completing lifecycle hook for instance {}: {}".format(
                self.instance_id, e.response['Error']['Code']))


# Argument Parser
def parse_command_line():
    parser = argparse.ArgumentParser(description='Begin graceful termination of EC2 instance.')
    parser.add_argument('--region', '-r', dest='region', required=True, help='The region.')
    parser.add_argument('--time', '-t', dest='time', required=True, help='The timestamp of the lifecycle hook.')
    parser.add_argument('--asg', '-a', dest='asg', required=True, help='The AutoScaling Group Name.')
    parser.add_argument('--instance', '-i', dest='instance', required=True, help='The EC2 Instance Id.')
    parser.add_argument('--lifecycle', '-l', dest='hook', required=True, help='The Lifecycle Hook Name.')
    parser.add_argument('--token', '-k', dest='token', required=True, help='The Lifecycle Action Token.')

    return parser.parse_args()


if __name__ == '__main__':

    args = parse_command_line()

    t = Terminator(region=args.region, time=args.time, asg=args.asg, instance=args.instance, hook=args.hook, token=args.token)
    queue_count = t.process_event()
    response = {}
    response["QueueCount"] = queue_count
    print json.dumps(response)
     # Stop Services
    t.service_controller('postfixStats', 'stop')
    t.service_controller('postfix', 'stop')
    if t.postfix_instance == 'postfix-is' or t.postfix_instance == 'postfix-cs':
        t.sqs_sns_cleanup()
    #t.cleanup_volume()
    sys.exit(queue_count)
