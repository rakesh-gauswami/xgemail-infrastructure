#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4

"""
Processes CloudWatch Event generated from Lifecycle Hook in order to perform an orderly shutdown of EC2 Instance.

Copyright 2021, Sophos Limited. All rights reserved.

'Sophos' and 'Sophos Anti-Virus' are registered trademarks of
Sophos Limited and Sophos Group.  All other product and company
names mentioned are trademarks or registered trademarks of their
respective owners.
"""

import argparse
import boto3
import botocore
import sys
import os
import json
import logging
import logging.handlers
import subprocess
from time import sleep
from datetime import datetime
from botocore.exceptions import ClientError


# logging to syslog setup
logger = logging.getLogger('instance-terminator')
logger.setLevel(logging.INFO)
syslog_handler = logging.handlers.SysLogHandler(address='/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(levelname)s %(message)s'
)
syslog_handler.formatter = formatter
logger.addHandler(syslog_handler)
delivery_instances = [
  'postfix-cd',
  'postfix-id',
  'postfix-ed',
  'postfix-rd',
  'postfix-wd',
  'postfix-bd',
  'postfix-dd',
  'postfix-mfid',
  'postfix-mfod'
]

class Terminator(object):

    def __init__(self, region, time, asg, instance, hook, token):

        self.logger = logging.getLogger('instance-terminator')
        self.postfix_instance = None
        self.aws_region = region
        self.timestamp = time
        self.delay = 5
        self.autoscaling_group_name = asg
        self.lifecycle_hook_name = hook
        self.lifecycle_action_token = token
        self.sqs_consumer_service = "<%= @sqs_consumer_service_name %>"
        self.postfix_queue_count = None
        self.postfix_queue_flushed = False

        # Create EC2 Resource
        self.ec2 = boto3.resource('ec2', region_name=self.aws_region)
        """:type: pyboto3.ec2 """
        # Create EC2 Client
        self.ec2c = self.ec2.meta.client
        """:type: pyboto3.ec2 """

        # Create SQS Client
        self.sqs = boto3.resource('sqs', region_name=self.aws_region)
        """:type: pyboto3.sqs """
        self.sqs_client = self.sqs.meta.client
        """:type: pyboto3.sqs """

        # Create SNS Client
        self.sns = boto3.resource('sns', region_name=self.aws_region)
        """:type: pyboto3.sns """
        self.sns_client = self.sns.meta.client
        """:type: pyboto3.sns """

        # Create SSM Client
        self.ssm = boto3.client('ssm', region_name=self.aws_region)
        """:type: pyboto3.ssm """

        self.instance = self.ec2.Instance(instance)

    def get_postfix_instance(self):
        """
        Return the Postfix Multi-Instance name.
        """
        output = self.run_cmd(
            ['postmulti', '-l'],
            'Getting Postfix Instance'
        )
        for line in output.split('\n'):
            if 'mta' in line:
                return line.split()[0]

    def run_cmd(self, cmd, comment=None):
        """
        Run a provided shell command.
        """
        if not cmd:
            self.logger.error("Function: run_cmd - Command not provided.")
            sys.exit(1)

        if comment:
            self.logger.info(comment)

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )

        out, err = process.communicate()

        if process.returncode != 0:
            self.logger.error("Exception while running command [ {} ], exiting.".format(cmd))
            sys.exit(process.returncode)

        return out.strip()

    def service_controller(self, service, action):
        """
        Used to control the Xgemail SQS Consumer Service.
        """
        process = subprocess.Popen(
            ('/sbin/service', service, action),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        out, err = process.communicate()
        self.logger.info(out.strip())

        return process.returncode

    def stop_sqs_consumer(self):
        """
        Stop SQS consumer (used on delivery instances)
        Returns True if the service is stopped or False if there is a problem
        Does it's best to detect problems and fix them.
        """
        if not self.sqs_consumer_service:
            self.logger.error("Function: stop_sqs_consumer - sqs_service not provided.")
            sys.exit(1)

        status = self.service_controller(self.sqs_consumer_service, 'status')
        if status == 0:
            self.logger.info("Xgemail SQS Consumer Service is running. Stopping Service.")
            if self.service_controller(self.sqs_consumer_service, 'stop') == 0:
                self.logger.info("Xgemail SQS Consumer Service is Stopped.")
                return True
        if (status == 1) or (status == 2):
            self.logger.warning("Xgemail SQS Consumer Service is dead but pid and lock files exist. Forcing a restart then stopping cleanly.")
            if self.service_controller(self.sqs_consumer_service, 'force-restart') == 0:
                self.logger.info("Xgemail SQS Consumer Service is Running. Stopping Service")
                if self.service_controller(self.sqs_consumer_service, 'stop') == 0:
                    self.logger.info("Xgemail SQS Consumer Service is Stopped.")
                    return True
        if status == 3:
            self.logger.info("Xgemail SQS Consumer Service is already Stopped.")
            return True

        self.logger.critical("CRITICAL! cannot stop Xgemail SQS Consumer!")
        return False

    def check_postfix_queue(self):
        """
        Check the postfix queue.
        Returns the number of messages in the Postfix queue
        """
        subdirs = ['maildrop', 'incoming', 'active', 'deferred']
        messages = 0

        for q in subdirs:
            path = ("/storage/{}/{}".format(self.postfix_instance, q))
            for root, dirs, files in os.walk(path):
                messages += len(files)
        return messages

    def backup_postfix_queue(self):
        """
        Copy all emails in the postfix queues to the EFS mount for recovery.
        After copy is verified, purge the postfix queue to return 0
        This will allow the volume to be deleted. Any failure will
        allow the volume to survive for manual recovery.
        """
        subdirs = ['maildrop', 'incoming', 'active', 'deferred']
        efstarget = ("/postfix-offline-storage/{}/{}".format(self.postfix_instance, self.timestamp))
        messages = 0
        efsmessages = 0

        self.logger.info("Copying {} messages to EFS mount {}".format(self.postfix_queue_count, efstarget))

        # Create a unique path on the EFS mount
        if not os.path.exists(efstarget):
            os.makedirs(efstarget)

        # Copy all emails from postfix queues to efs mount
        for q in subdirs:
            path = ("/storage/{}/{}".format(self.postfix_instance, q))
            for root, dirs, files in os.walk(path):
                messages += len(files)
            self.logger.info("Current queue progress: from {} to {}".format(path, efspath))
            self.run_cmd(['cp', '-rp', path, efstarget])
            efspath = ("{}/{}".format(efstarget, q))
            for root, dirs, files in os.walk(efspath):
                efsmessages += len(files)

        # Compare live queues to EFS queues
        if efsmessages == messages:
            self.logger.info("Finished copying messages from source: {} to EFS: {}".format(messages, efsmessages))
            # If all messages copied to EFS successfully then its safe to purge the queue
            self.run_cmd(['/usr/sbin/postmulti', '-i', self.postfix_instance, '-x', 'postsuper', '-d', 'ALL'])
            self.logger.info("Postfix queue has been purged {}".format(self.postfix_queue_count))
            return True
        else:
            self.logger.info("Postfix queue: {} was not successfully copied to EFS: {}".format(messages, efsmessages))
            return False

    def process_event(self):
        """
        Main function that processes the CloudWatch Event generated from the Lifecycle Hook.
        """

        self.logger.info("Begin processing Lifecycle Hook Event")
        self.service_controller('monit', 'stop')

        lifecycle_hook = LifecycleHook(region=self.aws_region, time=self.timestamp, asg=self.autoscaling_group_name, instance=self.instance.id, hook=self.lifecycle_hook_name, token=self.lifecycle_action_token)

        # Now perform all necessary logic to make sure the postfix server has empty queues before terminating.
        self.postfix_instance = self.get_postfix_instance()
        if self.postfix_instance in delivery_instances:
            if self.stop_sqs_consumer() is False:
                self.logger.error("Unable to continue. There is a problem with the Xgemail SQS Consumer Service.")
                sys.exit(1)

        self.postfix_queue_count = self.check_postfix_queue()
        self.logger.info("Messages in Postfix queue: %d" % self.postfix_queue_count)

        # If there are still messages in the Postfix queue
        while self.postfix_queue_count != 0:
            self.logger.warning("There are {} email messages still in the queue. We need to wait for them to drain.".format(self.postfix_queue_count))

            # First try to finish delivering all emails for 300X20 seconds
            for lifecycle_hook.heartbeat_request_count in range(1, 20, 1):
                while lifecycle_hook.time_check():
                    last_count = self.postfix_queue_count
                    sleep(10)

                    self.postfix_queue_count = self.check_postfix_queue()
                    self.logger.info("Queue Count: %d" % self.postfix_queue_count)

                    if self.postfix_queue_count == 0:
                        break

                    elif self.postfix_queue_count < last_count:
                        continue

                    elif self.postfix_queue_count >= last_count:
                      self.logger.warning("Email queue doesn't seem to be draining.")
                      if self.postfix_queue_flushed is False:
                          self.logger.info("Flushing queue")
                          self.run_cmd(['/usr/sbin/postmulti', '-i', self.postfix_instance, '-x', 'postqueue', '-f'])
                          self.postfix_queue_flushed = True
                      self.logger.info("Waiting 30 seconds before checking again.")
                      sleep(30)

                      self.postfix_queue_count = self.check_postfix_queue()

                      if self.postfix_queue_count < last_count:
                          self.logger.info("Email queue is draining. Continuing with regular check.")
                          continue

                      elif self.postfix_queue_count >= last_count:
                          self.logger.warning("Email queue is still not empty. Continuing with regular check.")
                          continue

                if self.postfix_queue_count == 0:
                    break

                self.logger.warning("Time Check - Time is running out!! Recording Lifecycle Action HeartBeat.")
                lifecycle_hook.record_lifecycle_action_heartbeat()

            if self.postfix_queue_count == 0:
                self.logger.info("The Postfix queue is empty.")
                return self.postfix_queue_count

            # If waiting doesn't deliver all email then copy it to EFS and delete the volume
            if self.backup_postfix_queue():
                self.logger.critical("LIFECYCLE HOOK EXHAUSTED! The postfix queue has been copied to EFS for recovery.")
                return self.postfix_queue_count
            else:
                error_message = "LIFECYCLE HOOK EXHAUSTED! *** {} *** Undelivered messages are stuck in the email queue.".format(self.postfix_queue_count)
                self.logger.critical(error_message)
                return self.postfix_queue_count


class LifecycleHook(object):

    def __init__(self, region, time, asg, instance, hook, token):
        self.logger = logging.getLogger('instance-terminator')
        self.aws_region = region
        # Create AS client
        self.asc = boto3.client('autoscaling', region_name=self.aws_region)
        """:type: pyboto3.autoscaling """
        self.aws_time_format = '%Y-%m-%dT%H:%M:%SZ'
        self.autoscaling_group_name = asg
        self.lifecycle_hook_name = hook
        self.instance_id = instance
        self.lifecycle_global_timeout = self.get_timeout('GlobalTimeout')
        self.heartbeat_request_count = 1
        self.heartbeat_timeout = self.get_timeout('HeartbeatTimeout')
        self.heartbeat_start_time = datetime.strptime(time, self.aws_time_format)
        self.lifecycle_action_token = token

    def time_check(self):
        """
        Calculates elapsed time: Current time in AWS format '%Y-%m-%dT%H:%M:%S.%fZ' minus the Heartbeat Start Time in AWS format.
        """
        seconds_buffer = 90
        elapsed_time = datetime.strptime(datetime.now().strftime(self.aws_time_format), self.aws_time_format) - self.heartbeat_start_time
        elapsed_time.total_seconds()
        self.logger.info("Time Check - Elapsed Time: {:04.1f} seconds, Heartbeat Timeout: {} seconds, Heartbeat Request Count: {}".format(elapsed_time.total_seconds(), self.heartbeat_timeout, self.heartbeat_request_count))
        if elapsed_time.total_seconds() <= self.heartbeat_timeout - seconds_buffer:
            return True
        else:
            return False

    def get_timeout(self, timeout):
        """
        Takes timeout parameter (GlobalTimeout or HeartbeatTimeout) and returns associated value from Describe Lifecycle Hooks.
        """
        try:
            response = self.asc.describe_lifecycle_hooks(
                AutoScalingGroupName=self.autoscaling_group_name,
                LifecycleHookNames=[self.lifecycle_hook_name]
            )['LifecycleHooks'][0][timeout]
            self.logger.info("{}: {}".format(timeout, response))
            return response
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception getting timeout for LifecycleHook {}: {}".format(
                self.lifecycle_hook_name, e.response['Error']['Code']))

    def record_lifecycle_action_heartbeat(self):
        """
        Records a heartbeat for the lifecycle action associated with the specified token or instance.
        This extends the timeout by the length of time defined using PutLifecycleHook.
        Resets the Heartbeat Start Time.
        """
        try:
            response = self.asc.record_lifecycle_action_heartbeat(
                LifecycleHookName=self.lifecycle_hook_name,
                AutoScalingGroupName=self.autoscaling_group_name,
                LifecycleActionToken=self.lifecycle_action_token,
                InstanceId=self.instance_id
            )
            self.heartbeat_start_time = datetime.strptime(datetime.now().strftime(self.aws_time_format), self.aws_time_format)
            self.logger.info("Recording Lifecycle Action Heartbeat: {} and resetting heartbeat start time to ".format(response, self.heartbeat_start_time))
            return response
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception recording lifecycle action heartbeat for instance {}: Lifecycle Hook Name {}: Lifecycle Action Token {}: {}".format(
                self.instance_id, self.lifecycle_hook_name, self.lifecycle_action_token, e.response['Error']['Code']))

    def complete_lifecycle_action(self):
        """
        Completes the lifecycle action for the specified token or instance with the specified result.
        """
        try:
            self.asc.complete_lifecycle_action(
                LifecycleHookName=self.lifecycle_hook_name,
                AutoScalingGroupName=self.autoscaling_group_name,
                LifecycleActionToken=self.lifecycle_action_token,
                LifecycleActionResult='CONTINUE',
            )
        except botocore.exceptions.ClientError as e:
            self.logger.exception("Exception completing lifecycle hook for instance {}: {}".format(
                self.instance_id, e.response['Error']['Code']))


# Argument Parser
def parse_command_line():
    parser = argparse.ArgumentParser(description='Begin graceful termination of EC2 instance.')
    parser.add_argument('--region', '-r', dest='region', required=True, help='The region.')
    parser.add_argument('--time', '-t', dest='time', required=True, help='The timestamp of the lifecycle hook.')
    parser.add_argument('--asg', '-a', dest='asg', required=True, help='The AutoScaling Group Name.')
    parser.add_argument('--instance', '-i', dest='instance', required=True, help='The EC2 Instance Id.')
    parser.add_argument('--lifecycle', '-l', dest='hook', required=True, help='The Lifecycle Hook Name.')
    parser.add_argument('--token', '-k', dest='token', required=True, help='The Lifecycle Action Token.')

    return parser.parse_args()


if __name__ == '__main__':

    args = parse_command_line()

    t = Terminator(region=args.region, time=args.time, asg=args.asg, instance=args.instance, hook=args.hook, token=args.token)
    queue_count = t.process_event()
    response = {}
    response["QueueCount"] = queue_count
    print json.dumps(response)
    t.service_controller('postfixStats', 'stop')
    t.service_controller('postfix', 'stop')
    sys.exit(queue_count)
