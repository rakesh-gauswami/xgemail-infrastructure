#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4
#
# Polls the delivery SQS and if a message is available, retrieves it and
# pulls the referenced email from S3. The last step is to inject the
# email into the local Postfix delivery queue.
#
# Copyright: Copyright (c) 1997-2017. All rights reserved.
# Company: Sophos Limited or one of its affiliates.

import sys
import time
sys.path.append("<%= @xgemail_utils_path %>")

import base64
import botocore
import boto3
from email.parser import Parser
import json
import logging
import messageformatter
import metadataformatter
import configformatter
import requests
import smtplib
import urllib
import re
import os.path
import messagehistory
import threading
import socket
from awshandler import AwsHandler
from common.sqsmessage import SqsMessage
from datetime import datetime
from queue_log import QueueLog
from sns_message_history_delivery_status import SnsMessageHistoryDeliveryStatus
from postfix_injection_response import PostfixInjectionResponse
from logging.handlers import SysLogHandler
from smtplib import SMTPDataError
from smtplib import SMTPException
from smtplib import SMTPSenderRefused
from smtplib import SMTPRecipientsRefused
from transportrouteconfig import TransportRouteConfig
from telemetrydataformatter import E2ETelemetryData

# general constants
AWS_REGION = "<%= @aws_region %>"
BUCKET_NAME = "<%= @s3_bucket_name %>"
MTA_HOST = "<%= @mta_host %>"
MTA_PORT = <%= @mta_port %>
PIC_FQDN = '<%= @xgemail_pic_fqdn %>'
SQS_MAX_NUMBER_OF_MESSAGES = <%= @sqs_max_number_of_messages %>
SNS_SQS_URL = "<%= @sns_sqs_url %>"
SQS_VISIBILITY_TIMEOUT = <%= @sqs_visibility_timeout %>
SQS_WAIT_TIME_SECONDS = <%= @sqs_wait_time_seconds %>
MAIL_PIC_HEADERS_API_RESPONSE_TIMEOUT_SECONDS = 5
MAIL_PIC_RESPONSE_TIMEOUT = <%= @mail_pic_api_response_timeout %>
MAIL_PIC_API_AUTH = '<%= @mail_pic_api_auth %>'
CONNECTIONS_BUCKET = '<%= @connections_bucket %>'
MESSAGE_DIRECTION = '<%= @message_direction %>'
MESSAGE_HISTORY_DELIVERY_STATUS_SNS_TOPIC = '<%= @message_history_status_sns_topic_arn %>'
E2E_LATENCY_TELEMETRY_DELIVERY_STREAM = '<%= @e2e_latency_telemetry_delivery_stream %>'
NODE_TYPE = '<%= @node_type %>'
NODE_IP = '<%= @node_ip %>'
ACCOUNT = '<%= @account %>'
POLICY_BUCKET_NAME = "<%= @policy_bucket %>"
TRANSPORT_CONFIG_PATH = "<%= @transport_config_path %>"
MH_MAIL_INFO_STORAGE_DIR = "<%= @mh_mail_info_storage_dir %>"
MSG_HISTORY_V2_BUCKET = "<%= @msg_history_v2_bucket_name %>"
DEFAULT_NO_OF_THREADS = <%= @default_number_of_consumer_threads %>
DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"

THREAD_COUNT_CONFIG_KEY = "<%= @consumer_thread_count_key %>"
X_SOPHOS_PROCESSING_START_TIME_HEADER = 'X-Sophos-Processing-Start-Time'
# logging to syslog setup
logging.getLogger("botocore").setLevel(logging.WARNING)
logger = logging.getLogger('sqsmsgconsumer')
logger.setLevel(logging.INFO)
handler = logging.handlers.SysLogHandler('/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(threadName)s %(levelname)s %(message)s'
)
handler.formatter = formatter
logger.addHandler(handler)

if ACCOUNT != 'sandbox':

  PIC_API_URL = 'https://%s/mail/api/xgemail' % (PIC_FQDN)

  def get_passphrase():
      s3 = boto3.client('s3')
      passphrase = s3.get_object(Bucket=CONNECTIONS_BUCKET, Key=MAIL_PIC_API_AUTH)
      return base64.b64encode('mail:' + passphrase['Body'].read())

  auth = get_passphrase()

  HEADERS = {
      'Content-type': 'application/json',
      'Authorization': 'Basic ' + auth
  }

else:

  HEADERS = {
      'Content-type': 'application/json',
      'Authorization': 'Basic'
  }

  PIC_API_URL = 'https://%s/mail-services/api/xgemail' % (PIC_FQDN)


# API constants
PIC_HEADERS_URL = '%s/headers' % (PIC_API_URL)
PIC_DESTINATION_URL = '%s/destination' % (PIC_API_URL)


awshandler = AwsHandler(AWS_REGION)
transportrouteconfig = TransportRouteConfig(TRANSPORT_CONFIG_PATH)

def parse_sqs_message(msg, receipt):
    msg_as_json = json.loads(msg)
    return SqsMessage(msg_as_json["schema_version"],
                      msg_as_json["message_path"],
                      msg_as_json["accepting_server_ip"],
                      msg_as_json["queue_id"],
                      msg_as_json["akm_key"],
                      msg_as_json["nonce"],
                      msg_as_json["message_key"],
                      msg_as_json["submit_message_type"],
                      receipt,
                      None,
                      msg_as_json['message_context'] if 'message_context' in msg_as_json else None)

# Attempts to create a new header of the format:
#
#   X-Sophos-Email-Transport-Route: <transport type>:[<host>]:<port>
#
# Where transport type may be either "smtp" or "smtp_encrypt",
# host is the ip address or fully qualified domain name of the
# customers mail server, and port is the port on which the mail
# server is listening.
#
# We retrieve the customers transport route from S3, and expect JSON in the format:
# {
#    "host": "host-name-or-ip-address",
#    "port": 25,
#    "transport": "<smtp or smtp_encrypt>"
# }
#
# If no transport route can be found or parsed, we refer to the transportrouteconfig to indicate what should happen.
# This will be one of three actions:
#
# 1. ERROR - Throw the error and cease processing
# 2. RETRIEVE - Attempt the retrieve the transport route from the mail PIC. Fail is this cannot be done.
# 3. IGNORE - Ignore the error and use "Unknown"
#
# The header name and value will be added to the existing_headers as a new key/value
def add_transport_route_header(recipients_addresses, existing_headers):

    header_name = 'X-Sophos-Email-Transport-Route'

    # All recipients should have the same domain at this point
    # due to upstream message splitting
    customer_domain = recipients_addresses[0].split('@')[1]

    try:

        s3_key = 'config/inbound-relay-control/delivery-routes/{0}.ROUTE'.format(customer_domain)

        raw_config_data = awshandler.download_data_from_s3(POLICY_BUCKET_NAME, s3_key)
        config_data = configformatter.get_config_binary(raw_config_data)
        config_json = json.loads(config_data)

        transport = config_json['transport']
        host = config_json['host']

        if 'port' in config_json:
            port = config_json['port']
            header_value = '{0}:{1}:{2}'.format(transport, host, port)
        else:
            header_value = '{0}:{1}'.format(transport, host)

        existing_headers[header_name] = header_value

    except Exception as route_exception:

        if transportrouteconfig.is_error_on_missing_data():

            logger.exception(
                'Unable to create transport route header for recipients [{0}], Exception: {1}'.format(
                    recipients,
                    route_exception
                )
            )

            raise route_exception

        fallback_header_value = 'Unknown'

        if transportrouteconfig.is_retrieve_on_missing_data():

            header_from_api = attempt_to_retrieve_transport_route(customer_domain, True)

            if header_from_api is not None:
                fallback_header_value = header_from_api
            else:
                # The route could not be retrieved from the API. Log the original exception so as to not swallow it,
                # then throw it so we cease processing and kick the job back to the queue. This is important.
                # If we can't create a real transport header, *and* postfix is configured to deliver using header checks
                # (which we expect it to be), then we can't inject the message into postfix otherwise delivery will
                # always fail. Therefore, if we're RETRIEVE mode, but the retrieve fails, we fail the message back to
                # SQS.
                logger.exception(
                    'Route for recipients [{0}] cannot be retrieved from S3 or destinations API. S3 exception: {1}'.format(
                        recipients_addresses,
                        route_exception
                    )
                )
                raise route_exception


        # This should only happen when the route data is not in S3, which should be never, however, we may see it time
        # to time. It should also be self-healing because calls to retrieve the data through the /destinations API will
        # push the data to S3 to prevent subsequent issues.
        logger.warn(
            'Unable to create transport route header for recipients [{0}], using fallback: {1}'.format(
                recipients_addresses,
                fallback_header_value
            )
        )

        existing_headers[header_name] = fallback_header_value


# Attempts to retrieve the transport route for the given domain from the mail PIC.
# If the API call is successful, this method should return a string in the format:
#
# <transport type>:[<host>]:<port>
#
# For example:
#
# smtp_encrypt:mail.customer-server.com:2255
#
# If the route cannot be retrieved for any reason, None is returned. This method also
# accepts a persist_if_missing boolean attribute which is passed on to the API.
# If true, the API should write the resulting transport data to S3 to prevent the need
# for subsequent API calls.
def attempt_to_retrieve_transport_route(domain, persist_if_missing):

    try:
        domain_response = requests.post(
            PIC_DESTINATION_URL,
            headers = HEADERS,
            data = json.dumps(
                {
                    'domain' : domain,
                    'persistIfMissing': persist_if_missing
                }
            ),
            timeout = MAIL_PIC_RESPONSE_TIMEOUT
        )

        if domain_response != requests.codes.ok:
            logger.warn("Unable to retrieve transport route for domain [%s]", domain)
            return None

        destination_json = domain_response.json()['delivery_destination']
        destination = destination_json['destination']
        port = destination_json['port']
        route_type = destination_json['route_type']

        if port != 25:
            route_val = '%s:[%s]' % route_type, destination

        else:
            route_val = '%s:[%s]:%s' % route_type, destination, str(port)

        return route_val

    except Exception as e:

        logger.exception(
            'Exception when retrieving transport route for domain [{0}], Exception: {1}'.format(domain, e)
        )

        return None


# Attempts to retrieve headers using api exposed on mail pic
def attempt_to_retrieve_headers(message_path):
    # encode the message_path query parameter with urllib to
    # avoid issues with special characters such as /
    query_parameters = {
        'message_path': urllib.quote_plus(message_path),
        'direction': MESSAGE_DIRECTION
    }

    logger.debug(
        'Retrieving headers from [%s] for message [%s] and query parameters %s',
        PIC_HEADERS_URL,
        message_path,
        query_parameters
    )

    if ACCOUNT != 'sandbox':
        response = requests.get(
            PIC_HEADERS_URL,
            headers=HEADERS,
            params = query_parameters,
            timeout = MAIL_PIC_HEADERS_API_RESPONSE_TIMEOUT_SECONDS
        )
    else:
        response = requests.get(
            PIC_HEADERS_URL,
            headers=HEADERS,
            params = query_parameters,
            timeout = MAIL_PIC_HEADERS_API_RESPONSE_TIMEOUT_SECONDS
        )

    response.raise_for_status()

    headers = response.json()['headers']

    return headers

# Accepts a postfix response in the form "2.0.0 Ok: queued as 412TZQ0wl9z1S"
# parses out and returns the queue id
def parse_postfix_queue_id_from_response(full_postfix_response):
    return re.sub('2.0.0\sOk:\squeued\sas\s', '', full_postfix_response)


# A recreation of the sendmail function from smtplib. This function does exactly the same thing
# as the sendmail function, however it returns a tuple of (failed_recipients, postfix_response) instead
# of just the failed recipients.
#
# You can view the original code by running the following on any CloudEmail instance:
#
############################################
# import smtplib
# import inspect
#
# server = smtplib.SMTP('127.0.0.1', 25)
# lines = inspect.getsource(server.sendmail)
# print lines
############################################
#
def sendmail_to_postfix(server, from_addr, to_addrs, msg, mail_options=[], rcpt_options=[]):
    server.ehlo_or_helo_if_needed()
    esmtp_opts = []

    if server.does_esmtp:
        if server.has_extn('size'):
            esmtp_opts.append("size=%d" % len(msg))
        for option in mail_options:
            esmtp_opts.append(option)

    (code, resp) = server.mail(from_addr, esmtp_opts)
    if code != 250:
        server.rset()
        raise SMTPSenderRefused(code, resp, from_addr)

    senderrs = {}
    if isinstance(to_addrs, basestring):
        to_addrs = [to_addrs]

    for each in to_addrs:
        (code, resp) = server.rcpt(each, rcpt_options)
        if (code != 250) and (code != 251):
            senderrs[each] = (code, resp)

    if len(senderrs) == len(to_addrs):
        # the server refused all our recipients
        server.rset()
        raise SMTPRecipientsRefused(senderrs)

    (code, resp) = server.data(msg)

    postfix_response = resp

    if code != 250:
        server.rset()
        raise SMTPDataError(code, resp)
    #if we got here then somebody got our mail
    return senderrs, postfix_response

# Adds headers to the provided message object.
# Returns the updated message object if successful. If any issues occurred with
# retrieving or updating the headers, the original message object is returned.
def add_headers(message, message_path, headers):
    try:
        if headers is None:
            logger.warn("Unable to add headers for message [%s]: header object was None.", message_path)
            return message , None

        if len(headers) == 0:
            logger.debug(
                "No headers found for message [%s]", message_path
            )
            return message , None

        # required to keep parsing with headers with true. Change here need to be tested with
        # DKIM Signing with Nested email
        parsed_message = Parser().parsestr(message, True)
        timestamp_value = parsed_message.get(X_SOPHOS_PROCESSING_START_TIME_HEADER)

        #Remove header 'X-Sophos-Processing-Start-Time' after reading it.
        parsed_message.__delitem__(X_SOPHOS_PROCESSING_START_TIME_HEADER)
        header_keys = []
        for header_key, header_value in headers.iteritems():
            header_keys.append(header_key)
            parsed_message[header_key] = header_value

        # required logging for DelayQueueTest, NonSpamEmailTest,
        # SandstormSasiDelayMRTest, SandstormSasiDelayTest
        logger.debug(
            'Added headers for message [%s]: %s',
            message_path,
            header_keys
        )

        return parsed_message.as_string(), timestamp_value

    except Exception:
        logger.exception("Unable to add headers for message [%s]", message_path)
        # return the original message without headers
        return message, None


# attempts to inject email into Postfix and
# returns True if successful, False otherwise.
def inject_email(sender, recipients, email, message_path, submit_queue_id):
    sendmail_errors = {}
    delivery_queue_id = None

    try:
        server = smtplib.SMTP(MTA_HOST, MTA_PORT)

        (sendmail_recipients_refused, raw_postfix_response) = sendmail_to_postfix(
            server,
            sender,
            recipients,
            email
        )

        delivery_queue_id = parse_postfix_queue_id_from_response(raw_postfix_response)

        logger.info(
            'Submit server queue id [%s] queued for delivery as [%s], message path [%s]',
            submit_queue_id,
            delivery_queue_id,
            message_path
        )

    except SMTPException:
        logger.exception("Errors during email injection for message with queue_id [%s] and message path [%s]",
                         submit_queue_id,
                         message_path)
        return PostfixInjectionResponse(False, None)
    finally:
        server.quit()

    # SMTP.sendmail() returns a dictionary of refused recipients.
    # At the time an email reaches the customer-delivery server,
    # rejecting any recipients should not happen. If it does happen,
    # we will log the rejected recipients and remove the message from SQS.
    if sendmail_recipients_refused:
        logger.error("Errors during email injection: {0}".format(sendmail_errors))
        return PostfixInjectionResponse(True, delivery_queue_id)
    return PostfixInjectionResponse(True, delivery_queue_id)

def create_and_publish_message_history_delivery_status_sns_job(sqs_message, postfix_injection_response):
    queue_log = QueueLog(
        sqs_message.schema_version,
        NODE_TYPE,
        NODE_IP,
        postfix_injection_response.nullable_delivery_queue_id,
        None,
        datetime.utcnow().strftime(DATETIME_FORMAT)
    )

    sns_message_history_delivery_status_job = SnsMessageHistoryDeliveryStatus(
        MESSAGE_DIRECTION,
        sqs_message.message_path,
        queue_log,
        None
    )

    try:
        awshandler.publish_to_sns_topic(
            MESSAGE_HISTORY_DELIVERY_STATUS_SNS_TOPIC,
            json.dumps(sns_message_history_delivery_status_job.get_sns_message_history_delivery_status_json())
        )

        logger.debug("Job <{0}> published to message history delivery status sns topic"
                     .format(sns_message_history_delivery_status_job))

    except Exception as e:
        logger.exception("Failed to publish event <{0}> to message history delivery status sqs queue"
                     .format(sns_message_history_delivery_status_job))

def is_multi_threaded():
    try:   
        #Check global flag.
        if awshandler.s3_key_exists(POLICY_BUCKET_NAME, GLOBAL_MULTITHREAD_ENABLED_KEY):
            return True
        #Check local flag.
        if os.path.isfile(LOCAL_MULTITHREAD_ENABLED_FILE_PATH):
            return True
    except Exception as e:
        logger.exception("Error in checking multi_thread config.[{0}]".format(e))
    # disabled by default.
    return False

def get_thread_count():
    try:
        if awshandler.s3_key_exists(POLICY_BUCKET_NAME, THREAD_COUNT_CONFIG_KEY):
            return int(awshandler.download_data_from_s3(POLICY_BUCKET_NAME, THREAD_COUNT_CONFIG_KEY))
    except Exception as e:
        logger.exception("Error in check thread count.[{0}]".format(e))
    # return default config.
    return DEFAULT_NO_OF_THREADS;

def wait_for_jilter()
    j = 0
    a_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    a_socket.settimeout(2.0)
    jilter_url = ("127.0.0.1", 9876)
    MAX_WAIT_SECONDS = 120
    while j < MAX_WAIT_SECONDS: # wait for 120s
        logger.info("[{}] Waiting for jilter to listen on port 9876".format(j))
        jilter_check = a_socket.connect_ex(jilter_url)
        if jilter_check == 0:
            logger.info("jilter process found on port 9876")
            break
        time.sleep(1)
        j = j + 1
    a_socket.close()

    if j == MAX_WAIT_SECONDS:
        logger.error("jilter process not found on port 9876. Exiting")
        exit(1)

def deliver_messages():
    while True:
        try:
            # doing a long poll to reduce number of empty responses.
            response = awshandler.receive_sqs_messages(
                SNS_SQS_URL,
                ['ApproximateReceiveCount'],
                [".*"],
                SQS_MAX_NUMBER_OF_MESSAGES,
                SQS_VISIBILITY_TIMEOUT,
                SQS_WAIT_TIME_SECONDS
            )
    
            if "Messages" not in response:
                # no messages found
                logger.debug("No new messages found in delivery SNS listener queue [{0}]".format(SNS_SQS_URL))
    
                continue
    
            for msg in response["Messages"]:
                if 'TopicArn' in msg['Body']:
                    # message came from SNS SQS queue
                    origin_sqs_queue = SNS_SQS_URL
                    msg_body = json.loads(msg['Body'])
                    sqs_message = parse_sqs_message(msg_body['Message'], msg['ReceiptHandle'])
                else:
                    # this allows us to handle message directly injected into the customer delivery sns listener queue
                    origin_sqs_queue = SNS_SQS_URL
                    sqs_message = parse_sqs_message(msg["Body"], msg["ReceiptHandle"])
    
                logger.debug("Polled SQS message [{0}] from queue [{1}]".format(sqs_message, origin_sqs_queue))
    
                headers = {}
                try:
                    headers = attempt_to_retrieve_headers(sqs_message.message_path)
                except Exception as e:
                    logger.warn("failed to retrieve headers at message path {0}, Exception {1}".format(sqs_message.message_path, e))
    
                try:
                    formatted_message = awshandler.download_message_from_s3(BUCKET_NAME, sqs_message)
                    message_binary = messageformatter.get_message_binary(formatted_message)
    
                    formatted_metadata = awshandler.download_metadata_from_s3(BUCKET_NAME, sqs_message)
                    metadata_raw = metadataformatter.get_metadata_binary(formatted_metadata)
                except botocore.exceptions.ClientError as e:
                    # this exception should never happen. It means that an SQS job
                    # was created but the email or metadata was not properly uploaded to S3
                    if e.response["Error"]["Code"] == "NoSuchKey":
                        logger.exception(
                            "Email or metadata [{0}] not found."
                                .format(sqs_message.message_path)
                        )
                    else:
                        logger.exception("Unexpected error during S3 object download.")
                    # for now, if any error occurs, we keep the SQS message.
                    # At a later point, we might decide to move the SQS message to a dead-letter queue.
                    continue
                except ValueError:
                    logger.exception("Exception during message/metadata parsing")
                    continue
    
                metadata = json.loads(metadata_raw)
                queue_id = metadata["queue_id"]
                sender = metadata["sender_address"]
                recipients = metadata["recipients"]
    
                # required logging for EncryptionServiceTest automation test
                logger.info(
                    "Email and metadata retrieved from S3. "
                    "QueueId: {0} - sender: {1} - recipients: [<{2}>]"
                        .format(
                        queue_id.encode('utf-8'),
                        None if sender is None else sender.encode('utf-8'),
                        '>, <'.join(r.encode('utf-8') for r in recipients)
                    )
                )
    
                mh_mail_info, can_generate_mh_event = messagehistory.get_mail_info(sqs_message, AWS_REGION, MSG_HISTORY_V2_BUCKET)
                if can_generate_mh_event:
                    logger.debug("QueueId [{0}] can generate message history event".format(queue_id.encode('utf-8')))
                    mh_mail_info_filename = None
                    try:
                        mh_mail_info_filename = messagehistory.write_mh_mail_info(
                            mh_mail_info,
                            MH_MAIL_INFO_STORAGE_DIR
                        )
                        logger.debug("written mh mail info file [{0}] for QueueId [{1}]".format(mh_mail_info_filename, queue_id.encode('utf-8')))
                    except Exception as e:
                        logger.error("Exception [{0}] while writing mh mail info for QueueId [{1}]".format(e, queue_id.encode('utf-8')))
    
                    # add mh_mail_info_filename to headers
                    if mh_mail_info_filename:
                        messagehistory.add_header(
                            mh_mail_info_filename,
                            headers
                        )
    
                # Add the transport route header to the existing headers if it's an INBOUND message
                if MESSAGE_DIRECTION == 'INBOUND':
                    add_transport_route_header(
                        recipients,
                        headers
                    )
    
                # attempt to add any existing headers to the message
                message_binary_with_headers, timestamp_header  = add_headers(
                    message_binary,
                    sqs_message.message_path,
                    headers
                )
    
                postfix_injection_response = inject_email(
                    sender,
                    recipients,
                    message_binary_with_headers,
                    sqs_message.message_path,
                    queue_id.encode('utf-8')
                )
    
                if postfix_injection_response.is_successfully_injected:
                    try:
                        logger.debug("Message with path: [{0}] having X-Sophos-Processing-Start-Time value : {1}"
                                     .format(sqs_message.message_path, timestamp_header))
    
                        #publish from normal delivery instances and skip from  all x delivery and encryption delivery
                        if timestamp_header is not None and NODE_TYPE in ['internet-delivery', 'customer-delivery', 'mf-inbound-delivery',
                                                                          'beta-delivery', 'delta-delivery',
                                                                          'risky-delivery', 'warmup-delivery',
                                                                          'mf-outbound-delivery']:
                            e2e_telemetry_data = E2ETelemetryData(queue_id,
                                                                  MESSAGE_DIRECTION,
                                                                  float(round(time.time() * 1000)) - float(timestamp_header),
                                                                  datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                            #put data to Kinesis firehose delivery stream
                            response = awshandler.put_data_to_kinesis_delivery_stream(
                                E2E_LATENCY_TELEMETRY_DELIVERY_STREAM,
                                e2e_telemetry_data.get_delivery_stream_json())
                            logger.debug("Response of delivery stream put operation from message path : [{}] is [{}]"
                                         .format(sqs_message.message_path, response))
                    except Exception as e:
                        logger.error("Error {0} occurred while pushing latency telemetry data for message path {1} "
                                     .format(sqs_message.message_path, e))
    
                    # Delete message from SQS after injecting into Postfix
                    awshandler.delete_message(
                        origin_sqs_queue,
                        sqs_message.receipt
                    )
    
                    logger.debug(
                        "Removed message [{0}] from SQS".format(sqs_message.message_path)
                    )
    
                    if NODE_TYPE != 'encryption-delivery':
                        create_and_publish_message_history_delivery_status_sns_job(
                            sqs_message,
                            postfix_injection_response
                        )
    
        except Exception:
            logger.exception("Unhandled exception in main loop")
            continue
    
# Wait for jilter to listen on port 9876
wait_for_jilter()



# Start consumers
i = 0;
thread_count = get_thread_count();
logger.info("Multi thread is enabled with thread count [{0}].".format(thread_count))
while i < thread_count:
    logger.info("Starting thread [{0}]".format(i))
    t = threading.Thread(target=deliver_messages)
    t.start()
    i = i + 1