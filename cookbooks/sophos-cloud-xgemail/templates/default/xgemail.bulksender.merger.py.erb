#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4
#
# This script consolidates all approved bulk senders into one file
#
# Copyright: Copyright (c) 1997-2017. All rights reserved.
# Company: Sophos Limited or one of its affiliates.

import sys
sys.path.append("<%= @xgemail_utils_path %>")
import os
import logging
import json
import formatterutils
import gziputils
import bulksenderformatter
from awshandler import AwsHandler
from logging import handlers
from botocore import exceptions

AWS_REGION = "<%= @aws_region %>"
EX_TEMPFAIL = <%= @temp_failure_code %>
POLICY_BUCKET_NAME = "<%= @policy_bucket %>"
S3_ENCRYPTION_ALGORITHM = "<%= @s3_encryption_algorithm %>"
BULK_SENDER_S3_PATH = "<%= @bulksender_s3_path %>"
BULK_SENDER_S3_RESTRUCTURE_PATH = "<%= @bulksender_s3_restructure_path %>"
MERGED_BULK_SENDER_FILENAME = "<%= @merged_bulksender_filename %>"

# logging to syslog setup
logger = logging.getLogger("bulksender-merger")
logger.setLevel(logging.INFO)
syslog_handler = logging.handlers.SysLogHandler(address="/dev/log")
formatter = logging.Formatter(
    "[%(name)s] %(process)d %(levelname)s %(message)s"
)
syslog_handler.formatter = formatter
logger.addHandler(syslog_handler)

awshandler = AwsHandler(AWS_REGION)

def get_approved_bulksenders_json(directory_objects):
    bulksender_json =  {
        'approved_bulksenders': list(directory_objects)
    }
    return json.dumps(bulksender_json)

def populate_approved_bulk_senders(bulk_sender_path, directory_objects):
    #get all approved files from S3
    approved_bulk_sender = awshandler.list_filtered_objects(
        POLICY_BUCKET_NAME,
        bulk_sender_path,
        ".APPROVED"
    )

    for filename in approved_bulk_sender:
        dir_object = os.path.splitext(os.path.basename(filename))[0]
        directory_objects.add(dir_object)

if __name__ == "__main__":
    try:

        
        directory_objects = set()

        #get all approved files from S3 old location.
        populate_approved_bulk_senders(BULK_SENDER_S3_PATH, directory_objects)

        #get all approved files from S3 new location.
        populate_approved_bulk_senders(BULK_SENDER_S3_RESTRUCTURE_PATH, directory_objects)

        #Upload file same policy bucket
        # magic_bytes, schema_version, nonce_length, bulk_sender_json):
        formatted_data = bulksenderformatter.get_formatted_bulk_senders(
            gziputils.gzip_data(
                get_approved_bulksenders_json(directory_objects)
            )
        )

        awshandler.upload_data_in_s3_without_expiration(
            POLICY_BUCKET_NAME,
            bulksenderformatter.get_s3_bulk_sender_path(
                "".join([BULK_SENDER_S3_PATH, MERGED_BULK_SENDER_FILENAME])
            ),
            formatted_data,
            S3_ENCRYPTION_ALGORITHM
        )

        # Once migration is complete we remove above call that writes to old  path.
        # This script runs once every hour in one CS server per region. Every time we write to both 
        # old and new location.
        awshandler.upload_data_in_s3_without_expiration(
            POLICY_BUCKET_NAME,
            bulksenderformatter.get_s3_bulk_sender_path(
                "".join([BULK_SENDER_S3_RESTRUCTURE_PATH, MERGED_BULK_SENDER_FILENAME])
            ),
            formatted_data,
            S3_ENCRYPTION_ALGORITHM
        )        
    except Exception as e:
        logger.exception("Unhandled exception in main ", e)
        exit(EX_TEMPFAIL)

