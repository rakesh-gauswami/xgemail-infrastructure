#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4
#
# This script provides a communication between Postfix and S3. It works as a local
# delivery agent to submit server which accepts an email from postfix and upload it
# to a designated S3 bucket. In case of a failure email stays in postfix deferred
# queue and postfix retries later.
#
# Copyright: Copyright (c) 1997-2018. All rights reserved.
# Company: Sophos Limited or one of its affiliates.

import sys
sys.path.append("<%= @xgemail_utils_path %>")

import copy
import formatterutils
import gzip
import gziputils
import hashlib
import io
import json
import logging
import messageformatter
import messagehistoryformatter
import metadataformatter
import multipolicyreaderutils
import os
import re
import signal
import uuidutils
import messagehistory
import mailinfoformatter
from awshandler import AwsHandler
from common.metadata import Metadata
from common.sqsmessage import SqsMessage
from common.messagehistoryevent import MessageHistoryEvent
from datetime import datetime
from datetime import timedelta
from email.parser import Parser
from logging.handlers import SysLogHandler
from routingmanager import RoutingManager
from scaneventattributes import ScanEventAttributes
from collections import OrderedDict
from recipientsplitconfig import RecipientSplitConfig

# Constants
AWS_REGION = "<%= @sqs_msg_producer_aws_region %>"
MSG_HISTORY_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_s3_bucket_name %>"
MSG_HISTORY_MS_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_ms_s3_bucket_name %>"
MSG_HISTORY_EVENTS_TOPIC_ARN = "<%= @sns_msg_history_events_sns_topic_arn %>"
POLICY_S3_BUCKET_NAME = "<%= @sqs_msg_producer_policy_s3_bucket_name %>"
PROCESS_TIMEOUT_SECONDS = <%= @sqs_msg_producer_process_timeout_seconds %>
POLICY_STORAGE_PATH = "<%= @policy_storage_path %>"

# Either 'INTERNET' or 'CUSTOMER'
SUBMIT_TYPE = "<%= @xgemail_submit_type %>"

SCAN_EVENTS_TOPIC_ARN = "<%= @sns_scan_events_sns_topic_arn %>"
S3_ENCRYPTION_ALGORITHM = "<%= @s3_encryption_algorithm %>"
SUBMIT_BUCKET_NAME = "<%= @sqs_msg_producer_s3_bucket_name %>"
SUBMIT_HOST_IP = "<%= @sqs_msg_producer_submit_ip %>"
SUBMIT_SQS_URL = "<%= @sqs_msg_producer_sqs_url %>"
SUBMIT_SERVICE_SQS_URL = "<%= @sqs_msg_producer_service_sqs_url %>"
TTL_IN_DAYS = <%= @sqs_msg_producer_ttl_in_days %>

MSG_HISTORY_V2_BUCKET = "<%= @msg_history_v2_bucket_name %>"
MSG_HISTORY_MAIL_INFO_MAX_BYTES_IN_MSG_CONTEXT = 20 * 1024
MSG_HISTORY_EVENT_DIR = "<%= @msg_history_event_dir %>"
MSG_HISTORY_EVENT_PROCESSOR_URL = "http://127.0.0.1:<%= @msg_history_event_processor_port %>/accept_event"

# Exit codes from sysexits
EX_TEMPFAIL = <%= @sqs_msg_producer_ex_temp_failure_code %>

#Message and metadata file details
BUFFER_SIZE = <%= @sqs_msg_producer_buffer_size %>
ROOT_DIR = "<%= @sqs_msg_producer_email_root_dir %>"
MAIL_FLOW_ROOT_DIR = ROOT_DIR + "/mailflow"
TIMESTAMP_FORMAT = "%Y/%m/%d/%H/%M"

#EmailProductType
GATEWAY = "Gateway"
MAILFLOW = "Mailflow"

# logging to syslog setup
logging.getLogger("botocore").setLevel(logging.WARNING)
logger = logging.getLogger('sqsmsgproducer')
logger.setLevel(logging.INFO)
handler = logging.handlers.SysLogHandler('/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(levelname)s %(message)s'
)
handler.formatter = formatter
logger.addHandler(handler)

# Encryption constants
# TODO: Need to be changed when Encryption is in place
AKM_KEY = "akm_key"
MESSAGE_KEY = "message_key"
NONCE = "nonce"
DOMAIN_NAME_REGEX_PATTERN = "[a-zA-Z0-9][-a-zA-Z0-9]*(\\.[-a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}"
EMAIL_ADDRESS_REGEX_PATTERN = "(([^<>()\[\]\\.,;:\s@\"]+(\.[^<>()\[\]\\.,;:\s@\"]+)*)|(\".+\"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))";

# Other constants
DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
MESSAGE_HISTORY_UNKNOWN_DESIGNATION = "UNKNOWN"
MESSAGE_HISTORY_ACCEPTED_EVENT = "ACCEPTED"
INBOUND_MESSAGE_DIRECTION = "INBOUND"
OUTBOUND_MESSAGE_DIRECTION = "OUTBOUND"
POLICY_ID_PATH_VALUE = "/00/"
SQS_BATCH_SIZE = 10
INTERNET = "INTERNET"
CUSTOMER = "CUSTOMER"
MF_INBOUND = "MF_INBOUND"
MF_OUTBOUND = "MF_OUTBOUND"
awshandler = AwsHandler(AWS_REGION)

routingmanager = RoutingManager(
    POLICY_STORAGE_PATH,
    SUBMIT_TYPE + "-SUBMIT"
)


#get email product type
def get_email_product_type():
    if SUBMIT_TYPE == MF_INBOUND or SUBMIT_TYPE == MF_OUTBOUND:
        return MAILFLOW
    else:
        return GATEWAY

#Strip the dot at the end of sender and/or recipients addresses as rfc:1034 mentions to consider ending dot optional
def strip_dot_at_end(email_address):
  if email_address.endswith("."):
    return email_address[:-1]
  else:
    return email_address

#postfix pipe sends metadata as sysargs.
def get_metadata(message_headers):
    try:
        metadata_length = len(sys.argv)
        if (metadata_length < 7):
            logger.info(" Usage: xgemail_sqs_message_producer.py <null_sender> "+
                         "<sender> <client_address> <queue_id> <domain> <original_recipient> ")
            exit(EX_TEMPFAIL)

        null_sender = sys.argv[1]
        sender_address = strip_dot_at_end(sys.argv[2])
        sender_ip = sys.argv[3]
        queue_id = sys.argv[4]
        recipient_domain = sys.argv[5]
        # TODO: see if we can pipe arrival date from postfix
        date_recorded = datetime.utcnow().strftime(DATETIME_FORMAT)
        # load rejected recipients from the file written by Jilter based on queue id
        rejected_recipients = get_rejected_recipients(queue_id, MSG_HISTORY_EVENT_DIR)
        recipient_list = [ ]
        for i in range (6,len(sys.argv)):
            recipient = strip_dot_at_end(sys.argv[i])
            if recipient.lower() in rejected_recipients:
                logger.info("silent dropping recipient [{0}] for queue id [{1}] due to rejection by jilter".format(recipient, queue_id))
            else:
                recipient_list.append(recipient)

        if (sender_address == null_sender):
            sender_address = None

        email_product_type = get_email_product_type()
        final_sender_ip = get_sender_ip_from_header(message_headers['X-Sophos-Sender-IP'], sender_ip, queue_id, email_product_type)
        x_sophos_header = uuidutils.get_x_sophos_email_id(message_headers['X-Sophos-Email-ID'], queue_id)

        metadata = Metadata(metadataformatter.SCHEMA_VERSION,
                            final_sender_ip,
                            sender_address,
                            SUBMIT_HOST_IP,
                            queue_id,
                            date_recorded,
                            recipient_domain,
                            recipient_list,
                            x_sophos_header,
                            False,
                            email_product_type)

        logger.debug("Created metadata info [{0}]".format(metadata))

        return metadata
    except Exception as e:
        logger.exception("Failed in preparing metadata [{0}]".format(e))
        exit(EX_TEMPFAIL)

def publish_to_topic(topic_arn, message_json, message_attributes = {}):
    attributes_json = {
        key: {'DataType': 'String', 'StringValue': value} for key, value in message_attributes.get_scan_event_attributes_json().items()
    }
    return awshandler.publish_to_sns_topic(
        topic_arn,
        json.dumps(message_json),
        attributes_json
    )

def get_sender_ip_from_header(ip_from_header, ip_from_postfix, queue_id, x_sophos_email_product_type):
    if x_sophos_email_product_type == GATEWAY:
        return ip_from_postfix
    if ip_from_header is None:
        logger.warning("Empty X-Sophos-Sender-IP header in message with queue_id: [{0}]".format(queue_id))
        return ip_from_postfix
    return ip_from_header

def add_to_sqs(sqs_url, sqs_json):
    return awshandler.add_to_sqs(
        sqs_url,
        json.dumps(sqs_json)
    )

def upload_to_s3(bucket_name, file_path, formatted_data, expires):
    return awshandler.upload_data_in_s3(
        bucket_name,
        file_path,
        formatted_data,
        expires,
        S3_ENCRYPTION_ALGORITHM
    )

# uploads message file to S3
def upload_message_to_s3(s3_file_path, expires, compressed_message):
    try:
        message_file_path = messageformatter.get_s3_message_path(
            s3_file_path
        )

        # required logging for XgemailTestBase automation test
        logger.info("Processing message [{0}]".format(message_file_path))

        #TODO: Encrypt mime message bytes after V1

        formatted_email_data = messageformatter.get_formatted_email_data(
            compressed_message
        )

        # upload message to the submit bucket
        upload_to_s3(
            SUBMIT_BUCKET_NAME,
            message_file_path,
            formatted_email_data,
            expires
        )

        logger.debug("Uploaded message to S3 [{0}]".format(message_file_path))

    except Exception as e:
        logger.exception("Failed in uploading message to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)

def upload_metadata_to_s3(s3_file_path, expires, metadata):
    try:
        metadata_file_path = metadataformatter.get_s3_metadata_path(
            s3_file_path
        )
        logger.debug("Processing metadata [{0}]".format(metadata_file_path))

        #TODO: Encrypt metadata object after V1

        # metadata_magic_bytes, schema_version, nonce_length, gzip_metadata_json):
        formatted_metadata = metadataformatter.get_formatted_metadata(
            gziputils.gzip_data(
                json.dumps(metadata.get_metadata_json())
            )
        )

        # upload metadata to submit bucket
        upload_to_s3(
            SUBMIT_BUCKET_NAME,
            metadata_file_path,
            formatted_metadata,
            expires
        )

        logger.debug("Uploaded metadata to S3 [{0}]".format(metadata_file_path))

    except Exception as e:
        logger.exception("Failed in uploading metadata to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


# publishes msg processing messages to topic
def publish_msg_processing_message(message_json, message_attributes = None, is_microservice_request = False):
    # Check if we should route to the microservice
    # Enforce that we only do this for internet submit for now.
    # This can be removed once we have an outbound microservice
    if 'INTERNET' == SUBMIT_TYPE and is_microservice_request:
        # publish to topic
        publish_to_topic(
            SCAN_EVENTS_TOPIC_ARN,
            message_json,
            message_attributes
        )
    else:
        send_msg_processing_sqs_message(message_json)


# sends msg processing sqs messages to sqs
def send_msg_processing_sqs_message(sqs_message_json):
    try:
        logger.debug("Processing SQS job [{0}]".format(sqs_message_json))
        add_to_sqs(SUBMIT_SQS_URL, sqs_message_json)
    except Exception as e:
        logger.exception("Failed in uploading message processing SQS job [{0}]".format(e))
        exit(EX_TEMPFAIL)

def get_plaintext_message():
    stdin_no = sys.stdin.fileno()
    plaintext_email_bytes = io.BytesIO()

    try:
        while True:
            try:
                new_bytes = os.read(stdin_no, BUFFER_SIZE)
                if not new_bytes:
                    break
                plaintext_email_bytes.write(new_bytes)
            except EOFError:
                break
            except Exception as e:
                logger.exception("Failed in parsing input email [{0}]".format(e))
                exit(EX_TEMPFAIL)
            except BaseException as e:
                logger.exception("Unexpected error in parsing input email: [{0}]".format(e))
                exit(EX_TEMPFAIL)
        return plaintext_email_bytes.getvalue()
    finally:
        plaintext_email_bytes.close()

def get_customer_id(queue_id, msghistory_events):
    try:
        if msghistory_events is not None and len(msghistory_events) > 0:
            accepted_event = msghistory_events.itervalues().next()
            return accepted_event['mail_info']['customer_id']
    except Exception as ex:
        logger.warning("Queue Id [{0}]. Error in getting customer id [{1}]".format(queue_id, ex))

# Inspects the headers of an outbound message and attempts to find the validated
# sender address by looking for policy data in S3.
#
# At this stage jilter will already have found a validated sender in either the
# From/Reply-To/x-sophos-effective-sender header,but this code has no way of knowing which one was used.
# So we look for policy data in S3 that matches a header (something jilter has already done).
#
# However, if x-sophos-effective-sender exists then it is clear that we can discard other two so we search against
# x-sophos-effective-sender header first to check Google groups or O365 auto-forward possibility
# if a validated sender cannot be found, we try the from header and then the Reply-To header.
# If a validated sender *still* cannot be found, we're forced to fail.
def get_metadata_for_outbound(metadata, headers, msghistory_events):

    validated_sender = None
    customer_id = get_customer_id(metadata.get_queue_id(), msghistory_events)
    if msghistory_events is not None and len(msghistory_events) > 0:
        # Get validated sender using MH accepted event file.
        logger.debug("Using message history to get sender for customer {0}".format(customer_id))
        validated_sender = list(msghistory_events.keys())[0]
    else:
        effective_sender_value = headers['x-sophos-effective-sender']
        effective_sender_header = 'x-sophos-effective-sender'

        if effective_sender_value is not None:
            return get_metadata_for_outbound_effective_sender(metadata, effective_sender_header, effective_sender_value)

 
        # Order is important as we want to evaluate the Sender, From and Reply-To headers in order
        headers_to_validate = OrderedDict()
        headers_to_validate['sender'] = headers['sender']
        headers_to_validate['from'] = headers['from']
        headers_to_validate['reply-to'] = headers['reply-to']

        

        for header_name, header_value in headers_to_validate.items():

            if header_value is not None:
                header_sender = get_address_from_header(header_value)

                is_valid_sender = is_valid_outbound_sender(header_sender, header_name)

                if is_valid_sender:
                    validated_sender = header_sender
                    break
                else:
                    logger.info("Invalid {0}:{1} in message with queue_id: [{2}]".format(
                        header_name,
                        header_value,
                        metadata.get_queue_id()
                        )
                    )

    if validated_sender is None:
        raise Exception(
            "No header sender address could be validated for message with queue id [{0}]".format(
                metadata.get_queue_id()
            )
        )

    return Metadata(
        metadata.get_schema_version(),
        metadata.get_sender_ip(),
        validated_sender,
        metadata.get_accepting_server_ip(),
        metadata.get_queue_id(),
        metadata.get_date_recorded(),
        metadata.get_recipient_domain(),
        metadata.get_recipients(),
        metadata.get_x_sophos_email_id(),
        metadata.is_microservice_request,
        metadata.get_email_product_type()
    )

# if get_metadata_for_outbound method finds x-sophos-effective-sender then this method returns the metadata
def get_metadata_for_outbound_effective_sender(metadata, header, header_value):

    header_sender = get_address_from_header(header_value)
    is_valid_sender = is_valid_outbound_sender(header_sender, header)

    if is_valid_sender:
        return Metadata(
            metadata.get_schema_version(),
            metadata.get_sender_ip(),
            header_sender,
            metadata.get_accepting_server_ip(),
            metadata.get_queue_id(),
            metadata.get_date_recorded(),
            metadata.get_recipient_domain(),
            metadata.get_recipients(),
            metadata.get_x_sophos_email_id(),
            metadata.is_microservice_request,
            metadata.get_email_product_type()
        )

    raise Exception(
        "No header sender address could be validated for message with queue id [{0}]".format(
            metadata.get_queue_id()
        )
    )

def is_valid_outbound_sender(header_sender, header_name):

    if header_sender is None or not header_sender:
        raise Exception("Header {0} cannot be null or empty in outbound".format(header_name))

    header_sender_policy_exists = multipolicyreaderutils.policy_file_exists_in_S3(
        header_sender.lower(),
        AWS_REGION, POLICY_S3_BUCKET_NAME
    )

    if not header_sender_policy_exists:
        header_sender_policy_exists = multipolicyreaderutils.outbound_relay_policy_file_exists_in_S3(
            header_sender.lower(),
            AWS_REGION, POLICY_S3_BUCKET_NAME
        )

    return header_sender_policy_exists

# Given an email address header value, such as:
#
# "First Last" <first.last@somewhere.com>
#
# Uses the Regex pattern we use in Jilter to match and return the email address portion of the header
def get_address_from_header(header_string):
    parsed_address = None

    match_object =  re.search(EMAIL_ADDRESS_REGEX_PATTERN, header_string)

    if match_object is None or not match_object:
        return parsed_address

    parsed_address = match_object.group()

    return parsed_address

def get_from_sender_domain(metadata):
    try:
        sender = metadata.get_sender_address()
        queue_id = metadata.get_queue_id()
        if sender is None or not sender:
            raise Exception("Invalid from header sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        domain = sender.split('@')[1]
        matched_string = re.match(DOMAIN_NAME_REGEX_PATTERN, domain)

        if matched_string is None or not matched_string:
            raise Exception("Invalid from sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        return domain
    except:
        raise Exception("Error in retrieving domain from from sender [{0}] for queue_id [{1}]".format(
            sender, queue_id)
        )


def upload_documents(metadata, compressed_message, s3_file_path):

    # prepared expiration date based on ttl_in_days
    expires = datetime.now() + timedelta(days=TTL_IN_DAYS)

    upload_message_to_s3(
        s3_file_path,
        expires,
        compressed_message
    )

    upload_metadata_to_s3(
        s3_file_path,
        expires,
        metadata
    )


def upload_msghistory_mail_info_to_s3(s3_file_path, msghistory_mail_info):
    try:
        msghistory_mail_info_path = mailinfoformatter.get_mail_info_path(
            s3_file_path
        )
        logger.debug("Processing message history [{0}]".format(msghistory_mail_info_path))

        formatted_mail_info = mailinfoformatter.get_formatted_mail_info(
            gziputils.gzip_data(
                json.dumps(msghistory_mail_info)
            ),
            msghistory_mail_info['schema_version']
        )
    
        # prepared expiration date based on ttl_in_days
        expires = datetime.now() + timedelta(days=TTL_IN_DAYS)

        # upload msg history to message history bucket
        upload_to_s3(
            MSG_HISTORY_V2_BUCKET,
            msghistory_mail_info_path,
            formatted_mail_info,
            expires
        )

        logger.debug("Uploaded mail info to S3 [{0}]".format(msghistory_mail_info_path))
        return msghistory_mail_info_path
    except Exception as e:
        logger.warning("S3 File Path [{0}]. Failed in uploading mail info to S3 [{1}]".format(s3_file_path, e))
    
def prepare_message_context(s3_file_path, mail_info):

    message_context = None

    #convert mail_info to str
    mail_info_str = json.dumps(mail_info)

    if len(mail_info_str) > MSG_HISTORY_MAIL_INFO_MAX_BYTES_IN_MSG_CONTEXT:
        mail_info_path = upload_msghistory_mail_info_to_s3(s3_file_path, mail_info)
        if mail_info_path is not None:
           message_context = {'mh_context' : { 'mail_info_s3_path' :  mail_info_path } }
    else:
        message_context = { 'mh_context' : { 'mail_info' : mail_info } }

    product_type = { 'email_product_type' : get_email_product_type() }
    message_context.update(product_type)

    return message_context

#The accepted events written by Jilter needs to be updated with 's3_file_path' and 'decorated_queue_id'
def update_msghistory_event_and_get_message_context(msghistory_events, s3_file_path, policy_metadata, direction, recipients, sender):
    try:
        mail_info = None
        mailbox = None

        messagehistory.update_msghistory_event(msghistory_events, s3_file_path, policy_metadata, direction, recipients, sender,get_email_product_type())

        if direction == INBOUND_MESSAGE_DIRECTION:
            mailbox = recipients[0].lower()
        else:
            mailbox = sender.lower()
        if (mailbox in msghistory_events and
            'mail_info' in msghistory_events[mailbox]):
            mail_info = msghistory_events[mailbox]['mail_info']
            return prepare_message_context(s3_file_path, mail_info)
        else:
            #For the given mailbox there was no accepted event in the file written by json.
            logger.warning("Queue Id [{0}]. Cannot find mail info for [{1}]. Message History Events[{2}]".format(
                    policy_metadata.get_queue_id(), mailbox, json.dumps(msghistory_events)
                )
            )
    except Exception as ex:
        logger.warning("Queue Id [{0}]. Error in processing message history event [{1}]".format(policy_metadata.get_queue_id(), ex))

def get_s3_path(queue_id, domain):
    root_dir = None
    if get_email_product_type() == MAILFLOW:
        root_dir = MAIL_FLOW_ROOT_DIR
    else:
        root_dir = ROOT_DIR

    s3_file_path = formatterutils.get_s3_path(
        root_dir,
        TIMESTAMP_FORMAT,
        SUBMIT_HOST_IP,
        queue_id,
        domain
    )
    return s3_file_path

def get_s3_prefix_path(queue_id, mailbox_id, domain):
    root_dir = None

    if get_email_product_type() == MAILFLOW:
        root_dir = MAIL_FLOW_ROOT_DIR
    else:
        root_dir = ROOT_DIR

    s3_file_path = formatterutils.get_s3_prefix_file_path(
        root_dir,
        queue_id,
        mailbox_id,
        SUBMIT_HOST_IP,
        domain
    )

    return s3_file_path


def base_policy_flow(metadata, direction, domain, sender, message, msghistory_events, message_attributes=None, policy=None):
    queue_id = metadata.get_queue_id()

    
    s3_file_path = get_s3_path(queue_id, domain)
    if multipolicyreaderutils.prefix_messages_path_enabled(get_customer_id(metadata.get_queue_id(), msghistory_events), SUBMIT_HOST_IP):
        s3_file_path = get_s3_prefix_path(queue_id, '00', domain)
   
    message_context = None
    if msghistory_events is not None:
      message_context = update_msghistory_event_and_get_message_context(msghistory_events, s3_file_path, metadata, direction, metadata.get_recipients(), sender)

    # create a sqs object
    sqs_message = SqsMessage(
        messageformatter.SCHEMA_VERSION,
        s3_file_path,
        SUBMIT_HOST_IP,
        queue_id,
        AKM_KEY,
        NONCE.encode('base64','strict'),
        MESSAGE_KEY.encode('base64','strict'),
        SUBMIT_TYPE,
        None,
        None,
        message_context
    )

    # required logging for EncryptionServiceTest automation test
    logger.info("Input email metadata info [{0}]".format(metadata))

    upload_documents(
        metadata,
        gziputils.gzip_data(message),
        s3_file_path
    )

    publish_msg_processing_message(
        sqs_message.get_sqs_json(),
        message_attributes,
        metadata.is_microservice_request
    )

    if msghistory_events is not None:
        messagehistory.handle_msghistory_events(queue_id, msghistory_events, MSG_HISTORY_EVENT_PROCESSOR_URL, MSG_HISTORY_EVENT_DIR)
    else:
        logger.warning("MH events not generated for Queue ID [{0}]".format(queue_id))

def build_recipient_map_from_msghistory_enabled(queue_id, recipients, msghistory_events):
    """
        This method returns a map with <mailbox_id> as key and recipient address as value.
    """
    recipient_map = {}
    for recipient in recipients:
        recipient = recipient.lower()
        if recipient in msghistory_events:
            #if alias exist for any email address then adding it in list for corresponding key
            if msghistory_events[recipient]['mail_info']['mailbox_id'] in recipient_map:
                recipient_map[msghistory_events[recipient]['mail_info']['mailbox_id']].append(recipient)
            else:
                recipient_map[msghistory_events[recipient]['mail_info']['mailbox_id']] = [recipient]
        else:
          # This should never happen
          logger.info("Queue Id [{0}] Accepted event not found for recipent [{1}]".format(queue_id, recipient))
          return None
    return recipient_map

def update_policy_context(message_context, recipient, policy):
    if policy is None or recipient is None or policy.get(recipient.lower()) is None:
        return message_context
    message_context.update({"policy":policy.get(recipient.lower())})
    return message_context

# Mail flow goes through multi_policy_flow if multi-policy flag is enabled
# but if there are error conditions such as file read error etc, the mail
# fall back to base_policy_flow.
def multi_policy_flow(metadata, direction, domain, sender, message, msghistory_events, message_attributes = None, policy = None):
    recipients = metadata.get_recipients()
    customer_id = get_customer_id(metadata.get_queue_id(), msghistory_events)
    policy_list = None
    if customer_id is not None and msghistory_events is not None and len(msghistory_events) > 0:
        logger.debug("Using message history to build recipient map for customer {0}".format(customer_id))
        policy_list = build_recipient_map_from_msghistory_enabled(metadata.get_queue_id(), recipients, msghistory_events)

    if customer_id is None or policy_list is None:
        logger.debug("Queue Id [{0}] Reading customer id from policy bucket".format(metadata.get_queue_id()))
        (policy_list, customer_id) = multipolicyreaderutils.build_policy_map(recipients, AWS_REGION, POLICY_S3_BUCKET_NAME)

    # Message routing information in metadata pertaining to micro-service
    # We are no longer planning to use mail-inbound micro-service.
    # So setting this to false by default. Saves two EFS reads.
    metadata.set_microservice_request(False)

    # required logging for EncryptionServiceTest automation test
    logger.info("Input email metadata info [{0}]".format(metadata))

    if not policy_list:
        return base_policy_flow(metadata, direction, domain, sender, message, message_attributes, policy)

    queue_id = metadata.get_queue_id()
    s3_file_path = get_s3_path(queue_id, domain)

    sqs_message_json_queue = []
    history_json_queue = []

    message_context = None

    for policy_id, recipient in policy_list.items():
        this_policy_metadata = copy.deepcopy(metadata)
        this_policy_metadata.set_recipients(recipient)
        this_policy_metadata.add_uuid_to_queue_id()

        policy_id_path = "/" + policy_id + "/"
        new_s3_file_path = s3_file_path.replace(POLICY_ID_PATH_VALUE, policy_id_path)
        if multipolicyreaderutils.prefix_messages_path_enabled(get_customer_id(queue_id, msghistory_events), SUBMIT_HOST_IP):
            new_s3_file_path = get_s3_prefix_path(queue_id, policy_id, domain)

        if msghistory_events is not None:
            message_context = update_msghistory_event_and_get_message_context(msghistory_events, new_s3_file_path, this_policy_metadata, direction, recipient, sender)

        if policy is not None:
            message_context = update_policy_context(message_context, recipient[0], policy)
        # create a sqs message with policy_id
        sqs_message = SqsMessage(
            messageformatter.SCHEMA_VERSION,
            new_s3_file_path,
            SUBMIT_HOST_IP,
            this_policy_metadata.get_queue_id(),
            AKM_KEY,
            NONCE.encode('base64','strict'),
            MESSAGE_KEY.encode('base64','strict'),
            SUBMIT_TYPE,
            None,
            policy_id,
            message_context
        )

        upload_documents(
            this_policy_metadata,
            gziputils.gzip_data(message),
            new_s3_file_path
        )

        #batch SQS job - every 10 jobs as a batch
        sqs_message_json_queue.append(sqs_message.get_sqs_json())
        if len(sqs_message_json_queue) == SQS_BATCH_SIZE:
            for processing_job in sqs_message_json_queue:
                publish_msg_processing_message(
                    processing_job,
                    message_attributes,
                    metadata.is_microservice_request
                )
            sqs_message_json_queue = []

    #send the remaining if any
    if len(sqs_message_json_queue) > 0:
        for processing_job in sqs_message_json_queue:
            publish_msg_processing_message(
                processing_job,
                message_attributes,
                metadata.is_microservice_request
            )

    if msghistory_events is not None:
        messagehistory.handle_msghistory_events(queue_id, msghistory_events, MSG_HISTORY_EVENT_PROCESSOR_URL, MSG_HISTORY_EVENT_DIR)
    else:
        logger.warning("MH events not generated for Queue ID [{0}]".format(queue_id))

def get_message_headers(message):
    return Parser().parsestr(message, headersonly=True)


def outbound_split_by_recipients(metadata, sender, domain, message, direction, msghistory_events, message_attributes=None, policy=None):

    # required logging for XgemailTestBase automation test
    logger.info("Metadata json info [{0}]".format(metadata))
    recipients = metadata.get_recipients()

    queue_id = metadata.get_queue_id()
    s3_file_path = get_s3_path(queue_id, domain)

    sqs_message_json_queue = []
    history_json_queue = []

    message_context = None

    for recipient in recipients:

        this_recipient_metadata = copy.deepcopy(metadata)
        this_recipient_metadata.set_recipients([recipient])
        this_recipient_metadata.add_uuid_to_queue_id()

        new_s3_file_path = s3_file_path.replace(POLICY_ID_PATH_VALUE,
                                                "/" + str(hashlib.sha256(str(recipient)).hexdigest()) + "/")

        if multipolicyreaderutils.prefix_messages_path_enabled(get_customer_id(queue_id, msghistory_events), SUBMIT_HOST_IP):
            new_s3_file_path = get_s3_prefix_path(queue_id, str(hashlib.sha256(str(recipient)).hexdigest()), domain)
                                            

        if msghistory_events is not None:
            message_context = update_msghistory_event_and_get_message_context(msghistory_events, new_s3_file_path, this_recipient_metadata, direction, this_recipient_metadata.get_recipients(), sender)

        if policy is not None:
            message_context = update_policy_context(message_context, recipient, policy)

        # create a sqs message with policy_id
        sqs_message = SqsMessage(
            messageformatter.SCHEMA_VERSION,
            new_s3_file_path,
            SUBMIT_HOST_IP,
            this_recipient_metadata.get_queue_id(),
            AKM_KEY,
            NONCE.encode('base64', 'strict'),
            MESSAGE_KEY.encode('base64', 'strict'),
            SUBMIT_TYPE,
            None,
            None,
            message_context
        )

        upload_documents(
            this_recipient_metadata,
            gziputils.gzip_data(message),
            new_s3_file_path
        )

        #batch SQS job - every 10 jobs as a batch
        sqs_message_json_queue.append(sqs_message.get_sqs_json())
        if len(sqs_message_json_queue) == SQS_BATCH_SIZE:
            for processing_job in sqs_message_json_queue:
                publish_msg_processing_message(
                    processing_job,
                    message_attributes,
                    metadata.is_microservice_request
                )
            sqs_message_json_queue = []

    #send the remaining if any
    if len(sqs_message_json_queue) > 0:
        for processing_job in sqs_message_json_queue:
            publish_msg_processing_message(
                processing_job,
                message_attributes,
                metadata.is_microservice_request
            )

    if msghistory_events is not None:
        messagehistory.handle_msghistory_events(queue_id, msghistory_events, MSG_HISTORY_EVENT_PROCESSOR_URL, MSG_HISTORY_EVENT_DIR)
    else:
        logger.warning("MH events not generated for Queue ID [{0}]".format(queue_id))

def get_rejected_recipients(queue_id, directory):
    """
    Reads the rejected recipients information written by Jilter
    """
    file_path = directory + '/' + queue_id + '.rejected'

    if not os.path.exists(file_path):
        return set()

    try:
        return set(line.strip().lower() for line in io.open(file_path, encoding='utf8'))
    except Exception as ex:
        logger.error("File path [{0}]. Error reading rejected recipients: [{1}]".format(file_path, ex))
        return set()

# accepts data from postfix and send message and metadata to S3 and
# a json object to SQS which provides pointer to the message in S3
def main():
    try:
        domain = None
        direction = None
        sender = None
        message_attributes = None
        plaintext_message = get_plaintext_message()
        message_headers = get_message_headers(plaintext_message)
        metadata = get_metadata(message_headers)
        queue_id = metadata.get_queue_id()
        #A dict with msghistory_events and policy
        jilter_context = messagehistory.read_jilter_context(queue_id, MSG_HISTORY_EVENT_DIR)
        msghistory_events = None
        policy = {}
        if jilter_context is not None and jilter_context.get('msghistory_events') is not None:
            msghistory_events=jilter_context.get('msghistory_events')
        if jilter_context is not None and jilter_context.get('policy') is not None and len(policy) > 0:
            policy = jilter_context.get('policy')

        if len(metadata.get_recipients()) == 0:
            logger.info("all recipients rejected by jilter for multiple reasons for metadata [{0}]".format(metadata))
            return

        if INTERNET == SUBMIT_TYPE:
            domain = metadata.get_recipient_domain()
            direction = INBOUND_MESSAGE_DIRECTION
            sender = metadata.get_sender_address()
            message_attributes = ScanEventAttributes(
                "internet-submit",
                "ACCEPTED",
                "v1"
            )
            
            return multi_policy_flow(metadata, direction, domain, sender, plaintext_message, msghistory_events, message_attributes, policy)

        elif MF_INBOUND == SUBMIT_TYPE:
            domain = metadata.get_recipient_domain()
            direction = INBOUND_MESSAGE_DIRECTION
            sender = metadata.get_sender_address()
            message_attributes = ScanEventAttributes(
                "mf-inbound-submit",
                "ACCEPTED",
                "v1"
            )
            
            return multi_policy_flow(metadata, direction, domain, sender, plaintext_message, msghistory_events, message_attributes, policy)

        elif CUSTOMER == SUBMIT_TYPE:
            direction = OUTBOUND_MESSAGE_DIRECTION
            metadata = get_metadata_for_outbound(metadata, message_headers, msghistory_events)
            sender = metadata.get_sender_address()
            domain = get_from_sender_domain(metadata)

            return outbound_split_by_recipients(metadata,sender,domain,plaintext_message,direction, msghistory_events, policy)

        elif MF_OUTBOUND == SUBMIT_TYPE:
            direction = OUTBOUND_MESSAGE_DIRECTION
            metadata = get_metadata_for_outbound(metadata, message_headers, msghistory_events)
            sender = metadata.get_sender_address()
            domain = get_from_sender_domain(metadata)

            return outbound_split_by_recipients(metadata,sender,domain,plaintext_message,direction, msghistory_events, policy)

        else:
            raise ("Wrong submit type [{0}]".format(SUBMIT_TYPE))

        # required logging for XgemailTestBase automation test
        logger.info("Metadata json info [{0}]".format(metadata))

        base_policy_flow(metadata, direction, domain, sender, plaintext_message, msghistory_events, message_attributes ,policy)
    except BaseException as e:
        logger.exception("Failed in processing email [{0}]".format(e))
        exit(EX_TEMPFAIL)
    finally:
        # disable any previously set alarm
        signal.alarm(0)

# Method is called when an alarm goes off. If that happens, then
# we temporarily fail processing which in turn will make Postfix
# queue the email for a later delivery attempt.
def signal_handler(signum, frame):
    logger.warning('Handling signal <{0}>. Processing took longer than {1}s to complete'.format(signum, PROCESS_TIMEOUT_SECONDS))
    exit(EX_TEMPFAIL)

if __name__ == "__main__":
    # The signal library is used to force a timeout on this producer script if needed.
    # See https://docs.python.org/2/library/signal.html for more information.
    # A timed out process will return the proper EX_TEMPFAIL back to Postfix
    # which will then properly retry the message at a later time.
    signal.signal(signal.SIGALRM, signal_handler)

    # set an alarm to go off after PROCESS_TIMEOUT_SECONDS
    signal.alarm(PROCESS_TIMEOUT_SECONDS)

    main()
