#!/usr/bin/env python
# vim: autoindent expandtab filetype=python shiftwidth=4 softtabstop=4 tabstop=4
#
# This script provides a communication between Postfix and S3. It works as a local
# delivery agent to submit server which accepts an email from postfix and upload it
# to a designated S3 bucket. In case of a failure email stays in postfix deferred
# queue and postfix retries later.
#
# Copyright: Copyright (c) 1997-2018. All rights reserved.
# Company: Sophos Limited or one of its affiliates.

import sys
sys.path.append("<%= @xgemail_utils_path %>")

import copy
import formatterutils
import gzip
import gziputils
import io
import json
import logging
import messageformatter
import messagehistoryformatter
import metadataformatter
import multipolicyreaderutils
import os
import re
import signal
from awshandler import AwsHandler
from common.metadata import Metadata
from common.sqsmessage import SqsMessage
from common.messagehistoryevent import MessageHistoryEvent
from datetime import datetime
from datetime import timedelta
from email.parser import Parser
from email.utils import parseaddr
from logging.handlers import SysLogHandler
from routingmanager import RoutingManager
from scaneventattributes import ScanEventAttributes

# Constants
AWS_REGION = "<%= @sqs_msg_producer_aws_region %>"
MSG_HISTORY_BUCKET_NAME = "<%= @sqs_msg_producer_msg_history_s3_bucket_name %>"
MSG_HISTORY_SQS_URL = "<%= @sqs_msg_producer_msg_history_sqs_url %>"
POLICY_S3_BUCKET_NAME = "<%= @sqs_msg_producer_policy_s3_bucket_name %>"
PROCESS_TIMEOUT_SECONDS = <%= @sqs_msg_producer_process_timeout_seconds %>
POLICY_STORAGE_PATH = "<%= @policy_storage_path %>"

# Either 'INTERNET' or 'CUSTOMER'
SUBMIT_TYPE = "<%= @xgemail_submit_type %>"

SCAN_EVENTS_TOPIC = "<%= @sns_scan_events_sns_topic %>"
S3_ENCRYPTION_ALGORITHM = "<%= @s3_encryption_algorithm %>"
SUBMIT_BUCKET_NAME = "<%= @sqs_msg_producer_s3_bucket_name %>"
SUBMIT_HOST_IP = "<%= @sqs_msg_producer_submit_ip %>"
SUBMIT_SQS_URL = "<%= @sqs_msg_producer_sqs_url %>"
SUBMIT_SERVICE_SQS_URL = "<%= @sqs_msg_producer_service_sqs_url %>"
TTL_IN_DAYS = <%= @sqs_msg_producer_ttl_in_days %>

# Exit codes from sysexits
EX_TEMPFAIL = <%= @sqs_msg_producer_ex_temp_failure_code %>

#Message and metadata file details
BUFFER_SIZE = <%= @sqs_msg_producer_buffer_size %>
ROOT_DIR = "<%= @sqs_msg_producer_email_root_dir %>"
TIMESTAMP_FORMAT = "%Y/%m/%d/%H/%M"

# logging to syslog setup
logging.getLogger("botocore").setLevel(logging.WARNING)
logger = logging.getLogger('sqsmsgproducer')
logger.setLevel(logging.INFO)
handler = logging.handlers.SysLogHandler('/dev/log')
formatter = logging.Formatter(
    '[%(name)s] %(process)d %(levelname)s %(message)s'
)
handler.formatter = formatter
logger.addHandler(handler)

# Encryption constants
# TODO: Need to be changed when Encryption is in place
AKM_KEY = "akm_key"
MESSAGE_KEY = "message_key"
NONCE = "nonce"
DOMAIN_NAME_REGEX_PATTERN = "[a-zA-Z0-9][-a-zA-Z0-9]*(\\.[-a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}"

# Other constants
DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
MESSAGE_HISTORY_UNKNOWN_DESIGNATION = "UNKNOWN"
MESSAGE_HISTORY_ACCEPTED_EVENT = "ACCEPTED"
INBOUND_MESSAGE_DIRECTION = "INBOUND"
OUTBOUND_MESSAGE_DIRECTION = "OUTBOUND"
POLICY_ID_PATH_VALUE = "/00/"
SQS_BATCH_SIZE = 10

awshandler = AwsHandler(AWS_REGION)

routingmanager = RoutingManager(
    POLICY_STORAGE_PATH,
    SUBMIT_TYPE + "-SUBMIT"
)

#postfix pipe sends metadata as sysargs.
def get_metadata():
    try:
        metadata_length = len(sys.argv)
        if (metadata_length < 7):
            logger.info(" Usage: xgemail_sqs_message_producer.py <null_sender> "+
                         "<sender> <client_address> <queue_id> <domain> <original_recipient> ")
            exit(EX_TEMPFAIL)

        null_sender = sys.argv[1]
        sender_address = sys.argv[2]
        sender_ip = sys.argv[3]
        queue_id = sys.argv[4]
        recipient_domain = sys.argv[5]
        # TODO: see if we can pipe arrival date from postfix
        date_recorded = datetime.utcnow().strftime(DATETIME_FORMAT)
        recipient_list = [ ]
        for i in range (6,len(sys.argv)):
            recipient_list.append(sys.argv[i])

        if (sender_address == null_sender):
            sender_address = None

        metadata = Metadata(metadataformatter.SCHEMA_VERSION,
                            sender_ip,
                            sender_address,
                            SUBMIT_HOST_IP,
                            queue_id,
                            date_recorded,
                            recipient_domain,
                            recipient_list)
        #logging metadata info
        logger.info("Input email metadata info [{0}]".format(metadata))
        return metadata
    except Exception as e:
        logger.exception("Failed in preparing metadata [{0}]".format(e))
        exit(EX_TEMPFAIL)

def publish_to_topic(topic_arn, message_json, message_attributes = {}):
    attributes_json = {
        key: {'DataType': 'String', 'StringValue': value} for key, value in message_attributes.get_scan_event_attributes_json.items()
    }
    return awshandler.publish_to_sns_topic(
        topic_arn,
        json.dumps(message_json),
        json.dumps(attributes_json)
    )

def add_to_sqs(sqs_url, sqs_json):
    return awshandler.add_to_sqs(
        sqs_url,
        json.dumps(sqs_json)
    )

def upload_to_s3(bucket_name, file_path, formatted_data, expires):
    return awshandler.upload_data_in_s3(
        bucket_name,
        file_path,
        formatted_data,
        expires,
        S3_ENCRYPTION_ALGORITHM
    )

# uploads message file to S3
def upload_message_to_s3(s3_file_path, expires, compressed_message):
    try:
        message_file_path = messageformatter.get_s3_message_path(
            s3_file_path
        )
        logger.info("Processing message [{0}]".format(message_file_path))

        #TODO: Encrypt mime message bytes after V1

        formatted_email_data = messageformatter.get_formatted_email_data(
            compressed_message
        )

        # upload message to the submit bucket
        upload_to_s3(
            SUBMIT_BUCKET_NAME,
            message_file_path,
            formatted_email_data,
            expires
        )

        logger.info("Uploaded message to S3 [{0}]".format(message_file_path))

    except Exception as e:
        logger.exception("Failed in uploading message to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


def upload_metadata_to_s3(s3_file_path, expires, metadata):
    try:
        metadata_file_path = metadataformatter.get_s3_metadata_path(
            s3_file_path
        )
        logger.info("Processing metadata [{0}]".format(metadata_file_path))

        #TODO: Encrypt metadata object after V1

        # metadata_magic_bytes, schema_version, nonce_length, gzip_metadata_json):
        formatted_metadata = metadataformatter.get_formatted_metadata(
            gziputils.gzip_data(
                json.dumps(metadata.get_metadata_json())
            )
        )

        # upload metadata to submit bucket
        upload_to_s3(
            SUBMIT_BUCKET_NAME,
            metadata_file_path,
            formatted_metadata,
            expires
        )

        logger.info("Uploaded metadata to S3 [{0}]".format(metadata_file_path))

    except Exception as e:
        logger.exception("Failed in uploading metadata to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


# uploads message history parent file to message history bucket
def upload_msg_history_to_s3(s3_file_path, expires, metadata):
    try:
        msg_history_file_path = messagehistoryformatter.get_s3_msg_history_path(
            s3_file_path
        )
        logger.info("Processing message history [{0}]".format(msg_history_file_path))

        #TODO: Encrypt msg history object after V1

        # metadata_magic_bytes, schema_version, nonce_length, gzip_metadata_json):
        formatted_msg_history = messagehistoryformatter.get_formatted_msg_history(
            gziputils.gzip_data(
                json.dumps(metadata.get_metadata_json())
            )
        )

        # upload msg history to message history bucket
        upload_to_s3(
            MSG_HISTORY_BUCKET_NAME,
            msg_history_file_path,
            formatted_msg_history,
            expires
        )

        logger.info("Uploaded message history to S3 [{0}]".format(msg_history_file_path))

    except Exception as e:
        logger.exception("Failed in uploading message history to S3 [{0}]".format(e))
        exit(EX_TEMPFAIL)


# Send an accepted message history event to msg history SQS
def prepare_msg_history_event(s3_file_path, metadata, sqs_message, direction, sender):
    try:
        # create a message history event object
        msg_history_event = MessageHistoryEvent(
            metadata.schema_version,
            s3_file_path,
            metadata.accepting_server_ip,
            metadata.queue_id,
            sqs_message.akm_key,
            sqs_message.nonce,
            sqs_message.message_key,
            datetime.utcnow().strftime(DATETIME_FORMAT),
            MESSAGE_HISTORY_ACCEPTED_EVENT,
            MESSAGE_HISTORY_UNKNOWN_DESIGNATION,
            None,
            metadata.recipients,
            False,
            direction,
            sender
        )

        return msg_history_event

    except Exception as e:
        logger.exception("Failed in uploading message history event to SQS [{0}]".format(e))
        exit(EX_TEMPFAIL)

def send_msg_history_sqs_event(sqs_history_json):
    try:
        logger.info("Processing message history event SQS job [{0}]".format(sqs_history_json))
        logger.info("Added message history event SQS job response [{0}]".format
            (add_to_sqs(
            MSG_HISTORY_SQS_URL,
            sqs_history_json)
        )
        )

    except Exception as e:
        logger.exception("Failed in uploading message processing SQS job [{0}]".format(e))
        exit(EX_TEMPFAIL)


# publishes msg processing messages to topic
def publish_msg_processing_message(message_json, direction, message_attributes = None, customer_id = None):
    # Check if we should route to the microservice
    # Enforce that we only do this for internet submit for now.
    # This can be removed once we have an outbound microservice
    if 'INTERNET' == SUBMIT_TYPE and routingmanager.perform_routing(customer_id):
        # publish to topic
        publish_to_topic(
            SCAN_EVENTS_TOPIC,
            message_json,
            message_attributes
        )
    else:
        send_msg_processing_sqs_message(message_json, direction)

# sends msg processing sqs messages to sqs
def send_msg_processing_sqs_message(sqs_message_json, customer_id = None):

    receiving_queue = SUBMIT_SQS_URL
    try:
        logger.info("Processing SQS job [{0}]".format(sqs_message_json))
        logger.info("Added SQS job response [{0}]".format
            (add_to_sqs(
                receiving_queue,
                sqs_message_json)
            )
        )
    except Exception as e:
        logger.exception("Failed in uploading message processing SQS job [{0}]".format(e))
        exit(EX_TEMPFAIL)


def get_plaintext_message():
    stdin_no = sys.stdin.fileno()
    plaintext_email_bytes = io.BytesIO()

    try:
        while True:
            try:
                new_bytes = os.read(stdin_no, BUFFER_SIZE)
                if not new_bytes:
                    break
                plaintext_email_bytes.write(new_bytes)
            except EOFError:
                break
            except Exception as e:
                logger.exception("Failed in parsing input email [{0}]".format(e))
                exit(EX_TEMPFAIL)
            except BaseException as e:
                logger.exception("Unexpected error in parsing input email: [{0}]".format(e))
                exit(EX_TEMPFAIL)
        return plaintext_email_bytes.getvalue()
    finally:
        plaintext_email_bytes.close()

def get_metadata_for_outbound(metadata, message):
    headers = Parser().parsestr(message, headersonly=True)

    from_header = headers['from']
    if from_header is None:
        raise Exception("Invalid from header in message with queue_id: [{0}]".format(
            metadata.get_queue_id())
        )

    # parseaddr always result into 2 tuple ['username', 'email address']
    # if parse fails then result will be ['','']
    parsed_from_address = parseaddr(from_header.strip().replace(',', ''))

    if parsed_from_address is None or len(parsed_from_address) != 2:
        raise Exception("Invalid parsed_from_address tuple: [{0}]".format(parsed_from_address))

    from_sender = parsed_from_address[1]
    logger.info("Parsed from header sender: [{0}]".format(from_sender))

    if from_sender is None or not from_sender:
        raise Exception("From header cannot be null or empty in outbound")

    return Metadata(
        metadata.get_schema_version(),
        metadata.get_sender_ip(),
        from_sender,
        metadata.get_accepting_server_ip(),
        metadata.get_queue_id(),
        metadata.get_date_recorded(),
        metadata.get_recipient_domain(),
        metadata.get_recipients()
    )


def get_from_sender_domain(metadata):
    try:
        sender = metadata.get_sender_address()
        queue_id = metadata.get_queue_id()
        if sender is None or not sender:
            raise Exception("Invalid from header sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        domain = sender.split('@')[1]
        matched_string = re.match(DOMAIN_NAME_REGEX_PATTERN, domain)

        if matched_string is None or not matched_string:
            raise Exception("Invalid from sender [{0}] for queue_id [{1}]".format(
                sender, queue_id)
            )

        return domain
    except:
        raise Exception("Error in retrieving domain from from sender [{0}] for queue_id [{1}]".format(
            sender, queue_id)
        )


def upload_documents(metadata, compressed_message, s3_file_path):

    # prepared expiration date based on ttl_in_days
    expires = datetime.now() + timedelta(days=TTL_IN_DAYS)

    upload_message_to_s3(
        s3_file_path,
        expires,
        compressed_message
    )

    upload_metadata_to_s3(
        s3_file_path,
        expires,
        metadata
    )

    upload_msg_history_to_s3(
        s3_file_path,
        expires,
        metadata
    )

def base_policy_flow(metadata, direction, domain, sender, message):
    queue_id = metadata.get_queue_id()

    s3_file_path = formatterutils.get_s3_path(
        ROOT_DIR,
        TIMESTAMP_FORMAT,
        SUBMIT_HOST_IP,
        queue_id,
        domain
    )

    # create a sqs object
    sqs_message = SqsMessage(
        messageformatter.SCHEMA_VERSION,
        s3_file_path,
        SUBMIT_HOST_IP,
        queue_id,
        AKM_KEY,
        NONCE.encode('base64','strict'),
        MESSAGE_KEY.encode('base64','strict'),
        SUBMIT_TYPE
    )

    upload_documents(
        metadata,
        gziputils.gzip_data(message),
        s3_file_path
    )

    send_msg_processing_sqs_message(
        sqs_message.get_sqs_json()
    )

    sqs_history = prepare_msg_history_event(
        s3_file_path,
        metadata,
        sqs_message,
        direction,
        sender
    )

    send_msg_history_sqs_event(
        sqs_history.get_sqs_json()
    )

# Mail flow goes through multi_policy_flow if multi-policy flag is enabled
# but if there are error conditions such as file read error etc, the mail
# fall back to base_policy_flow.
def multi_policy_flow(metadata, direction, domain, sender, message, message_attributes = None):
    recipients = metadata.get_recipients()
    (policy_list, customer_id) = multipolicyreaderutils.build_policy_map(recipients, AWS_REGION, POLICY_S3_BUCKET_NAME)

    # Message routing information in meatdata pertaining to micro-service
    metadata.set_microservice_request(routingmanager.perform_routing(customer_id))

    if not policy_list:
        return base_policy_flow(metadata, direction, domain, sender, message, message_attributes, customer_id)

    queue_id = metadata.get_queue_id()
    s3_file_path = formatterutils.get_s3_path(
        ROOT_DIR,
        TIMESTAMP_FORMAT,
        SUBMIT_HOST_IP,
        queue_id,
        domain
    )

    sqs_message_json_queue = []
    sqs_history_json_queue = []

    for policy_id, value in policy_list.items():
        this_policy_metadata = copy.deepcopy(metadata)
        this_policy_metadata.set_recipients(value)
        this_policy_metadata.add_uuid_to_queue_id()

        policy_id_path = "/" + policy_id + "/"
        new_s3_file_path = s3_file_path.replace(POLICY_ID_PATH_VALUE, policy_id_path)
        # create a sqs message with policy_id
        sqs_message = SqsMessage(
            messageformatter.SCHEMA_VERSION,
            new_s3_file_path,
            SUBMIT_HOST_IP,
            this_policy_metadata.get_queue_id(),
            AKM_KEY,
            NONCE.encode('base64','strict'),
            MESSAGE_KEY.encode('base64','strict'),
            SUBMIT_TYPE,
            None,
            policy_id
        )

        upload_documents(
            this_policy_metadata,
            gziputils.gzip_data(message),
            new_s3_file_path
        )

        #batch SQS job - every 10 jobs as a batch
        sqs_message_json_queue.append(sqs_message.get_sqs_json())
        if len(sqs_message_json_queue) == SQS_BATCH_SIZE:
            for processing_job in sqs_message_json_queue:
                publish_msg_processing_message(
                    processing_job,
                    direction,
                    message_attributes,
                    customer_id
                )
            sqs_message_json_queue = []

        # batch history sqs
        # noinspection CookbookSourceRoot
        sqs_history = prepare_msg_history_event(
            new_s3_file_path,
            this_policy_metadata,
            sqs_message,
            direction,
            sender
        )
        sqs_history_json_queue.append(sqs_history.get_sqs_json())
        if len(sqs_history_json_queue) == SQS_BATCH_SIZE:
            for history_job in sqs_history_json_queue:
                send_msg_history_sqs_event(
                    history_job
                )
            sqs_history_json_queue = []

    #send the remaining if any
    if len(sqs_message_json_queue) > 0:
        for processing_job in sqs_message_json_queue:
            publish_msg_processing_message(
                processing_job,
                direction,
                message_attributes,
                customer_id
            )

    if len(sqs_history_json_queue) > 0:
        for history_job in sqs_history_json_queue:
            send_msg_history_sqs_event(
                history_job
            )

# accepts data from postfix and send message and metadata to S3 and
# a json object to SQS which provides pointer to the message in S3
def main():
    try:
        metadata = get_metadata()
        domain = None
        direction = None
        sender = None
        message_attributes = None

        plaintext_message = get_plaintext_message()

        if 'INTERNET' == SUBMIT_TYPE:
            domain = metadata.get_recipient_domain()
            direction = INBOUND_MESSAGE_DIRECTION
            sender = metadata.get_sender_address()
            message_attributes = ScanEventAttributes(
                "internet-submit",
                "ACCEPTED",
                "v1"
            )
            if multipolicyreaderutils.get_multi_policy_enabled():
                return multi_policy_flow(metadata, direction, domain, sender, plaintext_message, message_attributes)

        elif 'CUSTOMER' == SUBMIT_TYPE:
            direction = OUTBOUND_MESSAGE_DIRECTION
            metadata = get_metadata_for_outbound(metadata, plaintext_message)
            sender = metadata.get_sender_address()
            domain = get_from_sender_domain(metadata)
        else:
            raise ("Wrong submit type [{0}]".format(SUBMIT_TYPE))

        logger.info("Metadata json info [{0}]".format(metadata))

        base_policy_flow(metadata, direction, domain, sender, plaintext_message, message_attributes)
    except BaseException as e:
        logger.exception("Failed in processing email [{0}]".format(e))
        exit(EX_TEMPFAIL)
    finally:
        # disable any previously set alarm
        signal.alarm(0)

# Method is called when an alarm goes off. If that happens, then
# we temporarily fail processing which in turn will make Postfix
# queue the email for a later delivery attempt.
def signal_handler(signum, frame):
    logger.info('Handling signal <{0}>. Processing took longer than {1}s to complete'.format(signum, PROCESS_TIMEOUT_SECONDS))
    exit(EX_TEMPFAIL)

if __name__ == "__main__":
    # The signal library is used to force a timeout on this producer script if needed.
    # See https://docs.python.org/2/library/signal.html for more information.
    # A timed out process will return the proper EX_TEMPFAIL back to Postfix
    # which will then properly retry the message at a later time.
    signal.signal(signal.SIGALRM, signal_handler)

    # set an alarm to go off after PROCESS_TIMEOUT_SECONDS
    signal.alarm(PROCESS_TIMEOUT_SECONDS)

    main()
